---
authors: ["brice.dutheil"]
date: "2020-05-23T23:45:29+02:00"
language: en
#tags: ["cgroup", "java", "kubernetes", "docker", "memory", "jcmd", "heap", "pmap"]
slug: "maxrampercentage-isnt-enough"
title: "MaxRamPercentage isn't enough"
draft: true
#_build:
#  list: never
---

So like many I was happy to see that the JDK 11 had everything to run
in a container, it can read information form the `cgroups` and it's possible to
tell the memory usage via the `-XX:MaxRAMPercentage`.
However...


.Problem
****
With a RAM percentage parameters at `85%` of `3 GB` memory, the container
application still go over this limit, which resulted in killed pods and failing
_deployment_ of our _replicaset_.

This issue started to appear with full traffic. (It was working well when
the traffic weight was 50%).
****

The following writing tries to gather various piece of information,
that are not really available if you don't know what to look for or where.
Also it summarize things I knew, things I grepped in the JDK codebase,
and things learned from other (awesome) people.

I hope I didn't dismissed important things, I hope I'm not totally wrong,
or simply I hope I'm not misguiding myself and especially others.

== Actual data from the prod cluster

The application runs on a Kubernetes cluster. And nice the team that handled the issue
wasn't really prepared, they bumped the memory limits of the application to 5 GB.


.deployment object of the app
[source,yaml]
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: edge-api
  # ...
spec:
  # ...
  template:
    # ...
    spec:
      containers:
      - image: eu.gcr.io/cd-registry/java-app:1.20200513.102101-4c53e9c
        name: java-app
        # ...
        resources:
          limits:
            cpu: "8"
            memory: 5Gi # <1>
          requests:
            cpu: "3"
            memory: 3Gi
        # ...
----
<1> Actual memory limit.

In order to understand our requirement, and to avoid lucky guess number (until it works),
let's explore the memory of a java process.


The RAM percentage are calculated at JVM startup from the given percentage, e.g.
with `-XX:InitialRAMPercentage=85.0` and `-XX:MaxRAMPercentage=85.0`) the JVM
calculates the following values.

.VM.flags in k8s
[source, bash]
----
$ jcmd $(pgrep java) VM.flags | tr ' ' '\n'
6:
-XX:CICompilerCount=4
-XX:ConcGCThreads=2
-XX:G1ConcRefinementThreads=8
-XX:G1HeapRegionSize=2097152
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=4563402752 <3>
-XX:InitialRAMPercentage=85.000000 <1>
-XX:+ManagementServer
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=4563402752 <4>
-XX:MaxNewSize=2736783360
-XX:MaxRAMPercentage=85.000000 <2>
-XX:MinHeapDeltaBytes=2097152
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5836300
-XX:NonProfiledCodeHeapSize=122910970
-XX:ProfiledCodeHeapSize=122910970
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC
----
<1> initial ram at 85%
<2> max ram at 85%
<3> initial heap size ~4.5GB
<4> max heap size ~4.5GB


=== Reading the memory footprint of the java process in the container

The first thing to look at is the _resident set size_, it can be obtained in
various way, e.g. using `ps` or reading the `/proc` should give the same number
if done at the same time.

.`ps`
[source, bash, role="primary"]
----
$ ps o pid,rss -p $(pidof java)
PID   RSS
  6 4701120
----

.`/proc/{pid}/status`
[source, bash, role="secondary"]
----
$ cat /proc/$(pgrep java)/status | grep VmRSS
VmRSS:	 4701120 kB
----

`4.7 GB` !!! Not quite within the 4.5 GB (85% of 5 GB) limit. So let's dig a bit to understand
this number (4701120 KB).

==== Digging in the java memory arenas

Fortunately the application started with `-XX:NativeMemoryTracking=summary` which
enables to have an overview of the different memory zones of a Java process.

NOTE: Enabling native memory tracking (NMT) causes a 5% to 10% performance overhead.

.`VM.native_memory` instant snapshot
[source, bash]
----
$ jcmd $(pgrep java) VM.native_memory scale=KB
6:

Native Memory Tracking:

Total: reserved=7168324KB, committed=5380868KB                               <1>
-                 Java Heap (reserved=4456448KB, committed=4456448KB)        <2>
                            (mmap: reserved=4456448KB, committed=4456448KB)

-                     Class (reserved=1195628KB, committed=165788KB)         <3>
                            (classes #28431)                                 <4>
                            (  instance classes #26792, array classes #1639)
                            (malloc=5740KB #87822)
                            (mmap: reserved=1189888KB, committed=160048KB)
                            (  Metadata:   )
                            (    reserved=141312KB, committed=139876KB)
                            (    used=135945KB)
                            (    free=3931KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=20172KB)
                            (    used=17864KB)
                            (    free=2308KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=696395KB, committed=85455KB)           <5>
                            (thread #674)
                            (stack: reserved=692812KB, committed=81872KB)
                            (malloc=2432KB #4046)
                            (arena=1150KB #1347)

-                      Code (reserved=251877KB, committed=105201KB)          <6>
                            (malloc=4189KB #11718)
                            (mmap: reserved=247688KB, committed=101012KB)

-                        GC (reserved=230739KB, committed=230739KB)          <7>
                            (malloc=32031KB #63631)
                            (mmap: reserved=198708KB, committed=198708KB)

-                  Compiler (reserved=5914KB, committed=5914KB)              <8>
                            (malloc=6143KB #3281)
                            (arena=18014398509481755KB #5)

-                  Internal (reserved=24460KB, committed=24460KB)           <10>
                            (malloc=24460KB #13140)

-                     Other (reserved=267034KB, committed=267034KB)         <11>
                            (malloc=267034KB #631)

-                    Symbol (reserved=28915KB, committed=28915KB)            <9>
                            (malloc=25423KB #330973)
                            (arena=3492KB #1)

-    Native Memory Tracking (reserved=8433KB, committed=8433KB)
                            (malloc=117KB #1498)
                            (tracking overhead=8316KB)

-               Arena Chunk (reserved=217KB, committed=217KB)
                            (malloc=217KB)

-                   Logging (reserved=7KB, committed=7KB)
                            (malloc=7KB #266)

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #521)

-                    Module (reserved=1362KB, committed=1362KB)
                            (malloc=1362KB #6320)

-              Synchronizer (reserved=837KB, committed=837KB)
                            (malloc=837KB #6877)

-                 Safepoint (reserved=8KB, committed=8KB)
                            (mmap: reserved=8KB, committed=8KB)

-                   Unknown (reserved=32KB, committed=32KB)
                            (mmap: reserved=32KB, committed=32KB)
----
<1> This shows a `reserved` value (`7168324 KB` (~`7.1 GB`)), it's the amount of addressable memory
(all OS types) on that container, and a `committed` value (`4456448 KB` (~`4.45 GB`)) that represents
what the JVM actually told the OS to allocate.
<2> Heap arena, note reserved and committed values are the same `4456448 KB` here because our
`InitialRAMPercentage` is the same as max. I'm not sure why this number is different from the VM
flags `-XX:MaxHeapSize=4563402752` though.
<3> ~`165 MB` of class metadata
<4> How many classes have been loaded : `28431`
<5> There are 674 threads that are using ~`81 MB` and could use up to `696 MB`.
<6> Code cache area (assembly of the used methods) ~105 MB out of 251 MB which matches with `-XX:ReservedCodeCacheSize=251658240`
<7> G1GC internal data structures take ~`230 MB`
<8> C1 / C2 compilers (which compile bytecodes to assembly) uses ~`6 MB`
<9> The symbols contains many things like interned strings and other internal constants ~`29 MB`
<10> Internal (includes `DirectByteBuffers` before Java 11), maybe others objects, here takes ~`24 MB`
<11> Other section after Java 11 includes `DirectByteBuffers` ~`267 MB`

Other areas are much smaller in scale, NMT takes ~`8 MB` itself, module system usage ~`1.3 MB`,
etc.
Also, note that enabling other part may show up if some JVM features are activated.
https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-5EF7BB07-C903-4EBD-A9C2-EC0E44048D37[Source]


[NOTE]
====
At the present moment NMT reports a committed memory of `5380868 KB` while process RSS is `4701120 KB`.
The difference relates to how `mmap` works (on Linux), memory pages are only backed by physical memory
once they're written to.

The Hotspot JVM option `-XX:+AlwaysPreTouch` can tell to always write zeroes to memory pages (which limits
memory commit latencies) used for the heap allocations. But other zones like thread stack works
differently, that means the some committed memory in NMT is not accounted by the RSS.

.vocabulary breakdown
[%autowidth.stretch]
|===

| *Used Heap* | The amount of memory occupied by live objects according.

| *Committed* | Address ranges that have been mapped with something other than `PROT_NONE`.
They may or may not be backed by physical or swap due to lazy allocation and paging.

| *Reserved* | The total address range that has been pre-mapped via `mmap` for a particular memory pool.
The _reserved_ / _committed_ difference consists of `PROT_NONE` mappings, which are guaranteed to not be backed
by physical memory

| *Resident* | Pages which are currently in physical ram. This means code, stacks, part of the committed memory
pools but also portions of ``mmap``ed files which have recently been accessed and allocations outside the control
of the JVM.

| *Virtual* | The sum of all virtual address mappings. Covers committed, reserved memory pools but also mapped
files or shared memory. This number is rarely informative since the JVM can reserve very large address
ranges in advance or mmap large files.

|===

https://stackoverflow.com/a/31178912/48136[source]
====

There's a lot more to read on the
https://docs.oracle.com/en/java/javase/11/vm/native-memory-tracking.html#GUID-39676837-DA61-4F8D-9C5B-9DB1F5147D80[official documentation about NMT]
and https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-1F53A50E-86FF-491D-A023-8EC4F1D1AC77[how to Monitor VM Internal Memory].

For a full blow deep dive read this article by http://twitter.com/shipilev[Aleksey Shipilёv] on
https://shipilev.net/jvm/anatomy-quarks/12-native-memory-tracking/[native memory tracking]


==== Explore what NMT does not show

There's also the `MappedByteBuffers`, these are the files mapped to virtual memory of a process.
NMT does not track them, however, `MappedByteBuffers` can also take physical memory. It's possible
to see the actual usage of a process memory map: `pmap -x <pid>`


.process memory mappings
[source, bash]
----
$ pmap -x $(pgrep java)
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
-XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
-Xlog:os,safepoint*,gc*,gc+ref=debug,gc+ergo*=debug,gc+age*=debug,gc+phases*:file=/gclogs/%t-gc.log:time,uptime,tags:filecount=5,filesize=10M -javaag
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
000000000216f000     404     272     272 rw---   [ anon ]
00000006f0000000 4476620 3128252 3128252 rw---   [ anon ]
00000008013b3000 1028404       0       0 -----   [ anon ]
00007fc5de9ea000      16       0       0 -----   [ anon ]
00007fc5de9ee000    1012     104     104 rw---   [ anon ]
00007fc5deaeb000      16       0       0 -----   [ anon ]
00007fc5deaef000    1012      24      24 rw---   [ anon ]
00007fc5debec000      16       0       0 -----   [ anon ]
00007fc5debf0000    1012      92      92 rw---   [ anon ]
00007fc5deced000      16       0       0 -----   [ anon ]
00007fc5decf1000    1012     100     100 rw---   [ anon ]
00007fc5dedee000      16       0       0 -----   [ anon ]
00007fc5dedf2000    1012     100     100 rw---   [ anon ]
00007fc5deeef000      16       0       0 -----   [ anon ]
00007fc5deef3000    1012     100     100 rw---   [ anon ]
00007fc5deff0000      16       0       0 -----   [ anon ]
00007fc5deff4000    1012     100     100 rw---   [ anon ]
00007fc5df0f1000      16       0       0 -----   [ anon ]
00007fc5df0f5000    1012     100     100 rw---   [ anon ]
00007fc5df1f2000      16       0       0 -----   [ anon ]
00007fc5df1f6000    1012     100     100 rw---   [ anon ]
00007fc5df2f3000      16       0       0 -----   [ anon ]
00007fc5df2f7000    1012     100     100 rw---   [ anon ]
00007fc5df3f4000      16       0       0 -----   [ anon ]
00007fc5df3f8000    1012     100     100 rw---   [ anon ]
00007fc5df4f5000      16       0       0 -----   [ anon ]
00007fc5df4f9000    1012     100     100 rw---   [ anon ]
00007fc5df5f6000      16       0       0 -----   [ anon ]
00007fc5df5fa000    1012     100     100 rw---   [ anon ]

...

00007fca48ba9000   17696   14876       0 r-x-- libjvm.so
00007fca49cf1000    2044       0       0 ----- libjvm.so
00007fca49ef0000     764     764     764 r---- libjvm.so
00007fca49faf000     232     232     208 rw--- libjvm.so
00007fca49fe9000     352     320     320 rw---   [ anon ]
00007fca4a041000     136     136       0 r---- libc-2.28.so
00007fca4a063000    1312    1140       0 r-x-- libc-2.28.so
00007fca4a1ab000     304     148       0 r---- libc-2.28.so
00007fca4a1f7000       4       0       0 ----- libc-2.28.so
00007fca4a1f8000      16      16      16 r---- libc-2.28.so
00007fca4a1fc000       8       8       8 rw--- libc-2.28.so
00007fca4a1fe000      16      16      16 rw---   [ anon ]
00007fca4a202000       4       4       0 r---- libdl-2.28.so
00007fca4a203000       4       4       0 r-x-- libdl-2.28.so
00007fca4a204000       4       4       0 r---- libdl-2.28.so
00007fca4a205000       4       4       4 r---- libdl-2.28.so
00007fca4a206000       4       4       4 rw--- libdl-2.28.so
00007fca4a207000     100     100       0 r-x-- libjli.so
00007fca4a220000    2048       0       0 ----- libjli.so
00007fca4a420000       4       4       4 r---- libjli.so
00007fca4a421000       4       4       4 rw--- libjli.so
00007fca4a422000      24      24       0 r---- libpthread-2.28.so
00007fca4a428000      60      60       0 r-x-- libpthread-2.28.so
00007fca4a437000      24       0       0 r---- libpthread-2.28.so
00007fca4a43d000       4       4       4 r---- libpthread-2.28.so
00007fca4a43e000       4       4       4 rw--- libpthread-2.28.so
00007fca4a43f000      16       4       4 rw---   [ anon ]
00007fca4a443000       4       4       0 r---- LC_IDENTIFICATION
00007fca4a444000       4       0       0 -----   [ anon ]
00007fca4a445000       4       0       0 r----   [ anon ]
00007fca4a446000       8       8       8 rw---   [ anon ]
00007fca4a448000       4       4       0 r---- ld-2.28.so
00007fca4a449000     120     120       0 r-x-- ld-2.28.so
00007fca4a467000      32      32       0 r---- ld-2.28.so
00007fca4a46f000       4       4       4 r---- ld-2.28.so
00007fca4a470000       4       4       4 rw--- ld-2.28.so
00007fca4a471000       4       4       4 rw---   [ anon ]
00007ffe28536000     140      40      40 rw---   [ stack ]
00007ffe28582000      12       0       0 r----   [ anon ]
00007ffe28585000       8       4       0 r-x--   [ anon ]
ffffffffff600000       4       0       0 r-x--   [ anon ]
---------------- ------- ------- -------
total kB         24035820 4776860 4720796
----

Let's refine that with more
https://www.kernel.org/doc/Documentation/filesystems/proc.txt[knowledge about `/proc/{pid}/maps`],
it indicates that a _map_ has a set of modes:

* `r-`: readable memory mapping
* `w`: writable memory mapping
* `x`: executable memory mapping
* `s` or `p` : shared memory mapping or private mapping. `/proc/<pid>/maps` shows both
but `pmap` only show the `s` flag.

On a side note, `pmap` may show another mapping mode which I barely found any reference of,
here's https://johanlouwers.blogspot.com/2017/07/oracle-linux-understanding-linux.html[one]
and https://linux.die.net/man/2/mmap[here]

* `R`: if set, the map has no swap space reserved (`MAP_NORESERVE` flag of `mmap`).
This means that we can get a segmentation fault by accessing that memory if it has not
already been mapped to physical memory, and if the system is out of physical memory.

At this time the focus is to see what are the memory mapped files with the JVM. Those can be either
read from or written to, we need to look for both the `r` or `w` or neither, also while quite unlikely
with Java let's not restrict on the _executable_ mapping, so the only thing we could be restricting to
is the shared mapping `s` (memory mapped files are shared because the OS may want to reuse the afferent
memory pages for other processes) :

.Our application memory mapped files
[source, bash]
----
$ pmap -x 6 | grep "[r-][w-][x-][s][R-]"
00007f5fdc02f000       4       4       0 r--s- instrumentation1647616515145161084.jar
00007f5fdc030000       4       4       0 r--s- instrumentation11262564974060761935.jar
00007f5fdc053000       8       8       0 r--s- java-agent-bs-cl.jar
00007f5fdc055000       4       4       0 r--s- instrumentation249633448216144460.jar
00007f5fdc056000       4       4       0 r--s- agent1-bootstrap10447345921091566771.jar
00007f5fdc057000      12      12       0 r--s- agent1-api6038277081136135384.jar
00007f5fec000000       8       8       0 r--s- agent1-weaver-api16247655721253674284.jar
00007f5fec002000       4       4       0 r--s- agent1-opentracing-bridge12060425782296980104.jar
00007f5fec003000      12      12       0 r--s- agent2-bridge3261511391751138774.jar
00007f5ffb910000  138176   36060       0 r--s- modules
00007f6008006000      28      28       0 r--s- gconv-modules.cache
                           ^^^^^               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
----

There's around `36 MB` of memory mapped files.

_As I was a bit unfamiliar with `pmap`, reading https://techtalk.intersec.com/2013/07/memory-part-2-understanding-process-memory/[this process memory blog]
helped me with the above command._

Wrapping this information from NMT and memory mapped files leaves us with the
following _equation_ to estimate the actual memory usage of a process:

....
Total memory = Heap + GC + Metaspace + Code Cache + Symbol tables
               + Compiler + Other JVM structures + Thread stacks
               + Direct buffers + Mapped files +
               + Native Libraries + Malloc overhead + ...
....

[%autowidth.stretch,options="footer"]
|===

| Heap                            | 4456448
| GC                              |  230739
| Metaspace                       |  165788
| Code Cache                      |  105201
| Symbol tables                   |   28915
| Compiler                        |    5914
| Other JVM structures
(Internal + NMT + smaller area)   |   24460 + 8433 + 217 + 7 + 19 + 1362 + 837 + 8 + 32
| Thread stacks                   |   85455
| Direct buffers (Other)          |  267034
| Mapped files                    |   36060 + 4 + 4 + 8 + 4 + 4 + 12 + 8 + 4 + 12 + 28
| Native Libraries                | unaccounted at this time
| Malloc overhead                 | accounted in NMT
| ...                             |

| Total                           | 5242880 KB
|===

`5186278 KB` is what this container is actually using, so way over the RSS (`4701120 KB`)
but also over the `5 GB` (`5242880 KB`) of the pod limit. Yet this pod is healthy and far from
the thresholds to be oom killed.

*So what I am missing here ?*

There a few considerations to understand :

* NMT shows _reserved_ and _committed_ values on each arenas,
+
[%autowidth.stretch]
|===

| `reserved` | this is the size that the OS guarantees to be available (but the
JVM didn't tell the OS to allocate this memory)
| `committed` | this size indicate the memory that the JVM allocated on the OS

|===
+
Each of these memory arenas are managed differently: `GC`, `Compiler` have the
same committed and reserved memory values while other zones have the ability to
shrink or grow for example `thread stacks` arenas reports
`85455 KB` but could take up to `696395 KB` if necessary, and theoretically
same as the heap.

* While the JVM did allocate this memory, Linux on x86 hardware uses virtual
memory with paging. More specifically Linux optimize actual physical memory
and only commit a page physically if this page is actually written to. In this
case the `Heap` arena in particular seems to benefit from this behavior as the JVM
allocated `4456448 KB`, but the actual RAM _resident set size_ usage of this memory
zone seems at this time is `3128252 KB`.

Where to look this number? While it's easy to get the RSS of a process, to understand
if the committed heap actually _resides_ on physical memory you need to use `pmap` or
inspect `/proc/{pid}/maps` or `/proc/{pid}/smaps`. You have to notice the one of the first
memory zone is quite big and about the size of the committed heap as shown in NMT. It's easier
to spot with `pmap -X` (capital `X`). _Note the below capture are from a different pod/process_.

.`pmap -x <pid>`
[source, role="primary"]
----
$ pmap -x $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
0000000001cfc000     412     224     224 rw---   [ anon ]
00000006f0000000 4477472 2944744 2944744 rw---   [ anon ] <1>
0000000801488000 1027552       0       0 -----   [ anon ]
00007f11b3744000   16388   16388   16388 rw---   [ anon ]
00007f11b4745000      16       0       0 -----   [ anon ]
00007f11b4749000   50688   49484   49484 rw---   [ anon ]
00007f11b78c9000    1536       0       0 -----   [ anon ]
00007f11b7a49000   32776   32776   32776 rw---   [ anon ]
00007f11b9a4b000      16       0       0 -----   [ anon ] <2>
00007f11b9a4f000    1012      24      24 rw---   [ anon ] <3>
00007f11b9b4c000      16       0       0 -----   [ anon ]
00007f11b9b50000    1012      92      92 rw---   [ anon ]
00007f11b9c4d000      16       0       0 -----   [ anon ]
00007f11b9c51000    1012     116     116 rw---   [ anon ]
...
----
<1> heap arena
<2> a thread guard pages
<3> a thread stack

.`pmap- X <pid>`
[source, role="secondary"]
----
$ pmap -X $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom -XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
         Address Perm   Offset Device   Inode     Size     Rss     Pss Referenced Anonymous LazyFree ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible Mapping
        00400000 r-xp 00000000  08:01 4054960        4       4       1          4         0        0              0              0               0    0       0      0           0 java
        00600000 r--p 00000000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        00601000 rw-p 00001000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        01cfc000 rw-p 00000000  00:00       0      412     224     224        224       224        0              0              0               0    0       0      0           0 [heap] <1>
       6f0000000 rw-p 00000000  00:00       0  4477472 2939592 2939592    2939592   2939592        0              0              0               0    0       0      0           0
       801488000 ---p 00000000  00:00       0  1027552       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4745000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4749000 rw-p 00000000  00:00       0    50688   49472   49472      49472     49472        0              0              0               0    0       0      0           0
    7f11b78c9000 ---p 00000000  00:00       0     1536       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b7a49000 rw-p 00000000  00:00       0    32776   32776   32776      32776     32776        0              0              0               0    0       0      0           0
    7f11b9a4b000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0        <2>
    7f11b9a4f000 rw-p 00000000  00:00       0     1012     112     112        112       112        0              0              0               0    0       0      0           0        <3>
    7f11b9b4c000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9b50000 rw-p 00000000  00:00       0     1012      96      96         96        96        0              0              0               0    0       0      0           0
    7f11b9c4d000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9c51000 rw-p 00000000  00:00       0     1012     116     116        116       116        0              0              0               0    0       0      0           0
...
----
<1> heap arena
<2> a thread guard pages
<3> a thread stack


== Going back to choose a better value for the RAM percentage

From the above, it's now possible with NMT especially and with `pmap` to
understand actual memory usage and to answer the question: "What is a sensible
RAM percentage setting for this application ?"

Really what drive the answer is the actual non-heap usage not accounted in `MaxRAMPercentage`, from
the numbers above:

....
(total) 5242880 - (heap) 4456448 = 786432 KB
....


.In percentages
[%autowidth.stretch,options="footer"]
|===

| Non heap | 5242880 - 4456448 = 786432 | ~14 %
| Heap     | 4456448                    | ~86 %

| Total    | 5186278                    | 100 %
|===

*This means the application needs around `790 MB`, plus the heap to run.*

From the flags seen above, the JVM set the heap maximum size memory to `4 563 402 752` Bytes,
this value was computed from this flag `-XX:MaxRAMPercentage=85.000000`, and this percentage
is somehow a lucky guess that worked for the `5 GB` deployment memory limit.
But this actual percentage is in fact _wrong_, if he JVM needed all the memory within the max
heap plus bigger stack traces then the container/pod would have been _oom killed_. Also, it is
necessary to give some free space in the container
to be able to perform serviceability tasks, like profiling, heap dump, etc.

For a 5GB limit it may be good to give around 20% for all of these non-heap, plus system space
for this particular workload (e.g. if the application requires heavy filesystem usage, then
it would be a different number to make room for the filesystem cache).

So the problem would be solved with the following value, for a `5 GB` memory limit :

[source]
----
-XX:InitialRAMPercentage=80.0 <1>
-XX:MaxRAMPercentage=80.0 <1>
----



For a quick win let's adapt the application image.

== Make the docker image memory settings tweakable per environment

AS seen at beginning of this post, RAM settings are part of the command declaration, this
is not suitable as seen above. In addition, the deployment requirements / limits is likely to
differ depending on the cluster / environment. One good reason would be to decrease the money spending
on your cloud provider for non-production clusters, like staging, pre-production, etc.
It will be useful to enable flexibility one setting the application for any given environment.

Let's use https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-0A40ECEE-AFDF-48CB-AF7C-A33DDE07A8DC[`JAVA_TOOL_OPTIONS`]
environment variable to enable flexibility and remove the RAM percentage in the `CMD` directive.

.Application dockerfile
[source,diff]
----
  ARG REGISTRY
  FROM $REGISTRY/corretto-java:11.0.6.10.1
+ ENV JAVA_TOOL_OPTIONS="" <1>

  RUN mkdir -p /gclogs /etc/java-app

  COPY ./build/libs/java-app-boot.jar \
    ./build/java-agents/agent-1.jar \
    ./build/java-agents/agent-2.jar \
    ./src/serviceability/*.sh \
    /

  CMD [ "/usr/bin/java", \
        "-Dfile.encoding=UTF-8", \
        "-Duser.timezone=UTC", \
        "-Dcom.sun.management.jmxremote.port=7199", \
        "-Dcom.sun.management.jmxremote.rmi.port=7199", \
        "-Dcom.sun.management.jmxremote.ssl=false", \
        "-Dcom.sun.management.jmxremote.authenticate=false", \
        "-Djava.security.egd=file:/dev/./urandom", \
-       "-XX:InitialRAMPercentage=85.0", \ <2>
-       "-XX:MaxRAMPercentage=85.0", \
        "-XX:NativeMemoryTracking=summary", \
        "-Xlog:os,safepoint*,gc*,gc+ref=debug,gc+ergo*=debug,gc+age*=debug,gc+phases*:file=/gclogs/%t-gc.log:time,uptime,tags:filecount=5,filesize=10M", \
        "-javaagent:/agent-1.jar", \
        "-javaagent:/agent-2.jar", \
        "-Dsqreen.config_file=/sqreen.properties", \
        "-jar", \
        "/java-app-boot.jar", \
        "--spring.config.additional-location=/etc/java-app/config.yaml", \
        "--server.port=8080" ]

  LABEL name="java-app"
  LABEL build_path="../"
  LABEL version_auto_semver="true"
----
<1> Defines the https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-0A40ECEE-AFDF-48CB-AF7C-A33DDE07A8DC[`JAVA_TOOL_OPTIONS`]
<2> Removes the RAM percentage settings to get _default_ values.

Now let's test this locally to play a bit.

.Build the container
[source]
----
❯ DOCKER_BUILDKIT=1 docker build \
  --tag test-java-app \ <1>
  --build-arg REGISTRY=eu.gcr.io/cd-registry \
  --file _infra/Dockerfile \
  .
[+] Building 1.4s (9/9) FINISHED
 => [internal] load build definition from Dockerfile                                                                                              0.0s
 => => transferring dockerfile: 1.34kB                                                                                                            0.0s
 => [internal] load .dockerignore                                                                                                                 0.0s
 => => transferring context: 35B                                                                                                                  0.0s
 => [internal] load metadata for eu.gcr.io/cd-registry/corretto-java:11.0.6.10.1                                                                  0.0s
 => CACHED [1/4] FROM eu.gcr.io/cd-registry/corretto-java:11.0.6.10.1                                                                             0.0s
 => [internal] load build context                                                                                                                 0.0s
 => => transferring context: 1.32kB                                                                                                               0.0s
 => [2/4] RUN mkdir -p /gclogs /etc/java-app                                                                                                      0.3s
 => [3/4] COPY ./build/async-profiler/linux-x64 /async-profiler                                                                                   0.0s
 => [4/4] COPY ./build/libs/java-app-boot.jar   ./build/java-agents/agent-1.jar   ./build/java-agents/agent-2.jar   ./src/serviceability/*.sh   / 0.6s
 => exporting to image                                                                                                                            0.4s
 => => exporting layers                                                                                                                           0.4s
 => => writing image sha256:5ceef8f5a4e23cb3bea7ca7cb7c90c0e338386b7f37992c92861cb119c312cb9                                                      0.0s
 => => naming to docker.io/library/test-java-app
----
<1> Custom tag to avoid collision with regular images in my cache

Run the container with the Java app

.*Without* `JAVA_TOOL_OPTIONS`
[source,role="primary"]
----
❯ docker run --rm --memory="3gb" --name j-mem test-java-app
Picked up JAVA_TOOL_OPTIONS:
10:14:53.566 [main] INFO org.springframework.core.KotlinDetector - Kotlin reflection implementation not found at runtime, related features won't be available.
2020-03-20 10:14:55.616 [] WARN  --- [kground-preinit] o.s.h.c.j.Jackson2ObjectMapperBuilder    : For Jackson Kotlin classes support please add "com.fasterxml.jackson.module:jackson-module-kotlin" to the classpath
...
----

.*With* `JAVA_TOOL_OPTIONS`
[source,role="secondary"]
----
❯ docker run --rm --memory="3gb" --env JAVA_TOOL_OPTIONS="-XX:InitialRAMPercentage=70.0 -XX:MaxRAMPercentage=70.0" --name j-mem test-java-app
Picked up JAVA_TOOL_OPTIONS: -XX:InitialRAMPercentage=70.0 -XX:MaxRAMPercentage=70.0
10:14:53.566 [main] INFO org.springframework.core.KotlinDetector - Kotlin reflection implementation not found at runtime, related features won't be available.
2020-03-20 10:14:55.616 [] WARN  --- [kground-preinit] o.s.h.c.j.Jackson2ObjectMapperBuilder    : For Jackson Kotlin classes support please add "com.fasterxml.jackson.module:jackson-module-kotlin" to the classpath
...
----


Then we can make sure we have the correct flags.

.*Without* `JAVA_TOOL_OPTIONS`
[source, role="primary"]
----
❯ docker exec -it j-mem bash -c "jcmd \$(pgrep java) VM.flags | tr ' ' '\n'"
6:
-XX:CICompilerCount=3
-XX:ConcGCThreads=1
-XX:G1ConcRefinementThreads=4
-XX:G1HeapRegionSize=1048576
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=50331648
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=805306368 <1>
-XX:MaxNewSize=482344960
-XX:MinHeapDeltaBytes=1048576
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5830732
-XX:NonProfiledCodeHeapSize=122913754
-XX:ProfiledCodeHeapSize=122913754
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC

----
<1> Max heap is about `805 MB`

.*With* `JAVA_TOOL_OPTIONS`
[source, role="secondary"]
----
❯ docker exec -it j-mem bash -c "jcmd \$(pgrep java) VM.flags | tr ' ' '\n'"
6:
-XX:CICompilerCount=3
-XX:ConcGCThreads=1
-XX:G1ConcRefinementThreads=4
-XX:G1HeapRegionSize=1048576
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=2256535552
-XX:InitialRAMPercentage=70.000000
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=2256535552 <1>
-XX:MaxNewSize=1353711616
-XX:MaxRAMPercentage=70.000000
-XX:MinHeapDeltaBytes=1048576
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5830732
-XX:NonProfiledCodeHeapSize=122913754
-XX:ProfiledCodeHeapSize=122913754
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC

----
<1> Max heap is about `2.25 GB`


Notice when there's no RAM settings the JVM computed computed the max heap size at 25%
of memory constraints `3 GB`. And to 80%, `2.25 GB`, of the same limit when passing the RAM percentages.
Also the heap values are the only one affected, other memory areas default values kept the same values.


== Going further

As a reminder this application was set up with 85% max heap when the
deployment limit was `3 GB`, it worked well under 50% of the traffic but failed with full traffic.
Then this pod memory limit was bumped to `5 GB` and the pod wasn't anymore oomkilled.
How this _limit_ was found is a lucky guess, given the RAM percentages were set in the `CMD`
directive of the Dockerfile.

As identified above there are two, maybe three arenas whose usage may explain the surge in memory before
the memory limit was increased. I don't have anything to back that except how I expect these memory arenas
to grow but not the others.

1. The thread stack memory arena, the increase actual memory pages is small, but enough to be mentioned.

2. The GC internal memory arena, with more threads there are more allocations, and as such more
things to track.

3. The _other_ memory arenas with more `DirectByteBuffers` usage.

The heap had a max value anyway, and if it was then the app would either trigger full gcs, or self terminated
with an `OutOfMemoryError`, so that not the heap. AS for the offers it's unlikely with the workload they grow
that much.

My hypothesis is that when full traffic came to this pod, these arenas grew by `100 MB` to `200 MB` (sum),
while not much, it was sufficient to go over the 15% of memory left for the non heap memory, and thus triggered
the system oom killer.


Also, at some point in time this application worked well under way less memory in a different cluster `-Xmx=2g`.
The code is not the culprit in this case. Let's explore that.

=== Actual Java Heap usage

While the previous section allowed to understand the actual memory usage, it didn't give any figure
regarding the actual heap usage for this application :

.local docker
[source, role="primary"]
----
$ jcmd $(pgrep java) GC.heap_info
6:
 garbage-first heap   total 2203648K, used 373495K [0x0000000779800000, 0x0000000800000000)
  region size 1024K, 217 young (222208K), 8 survivors (8192K)
 Metaspace       used 103178K, capacity 105394K, committed 105604K, reserved 1140736K
  class space    used 13168K, capacity 14004K, committed 14060K, reserved 1048576K
----

.prod first
[source, role="secondary"]
----
$ jcmd $(pgrep java) GC.heap_info
6:
 garbage-first heap   total 4456448K, used 925702K [0x00000006f0000000, 0x0000000800000000)
  region size 2048K, 387 young (792576K), 12 survivors (24576K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----

.prod second
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 1245902K [0x00000006f0000000, 0x0000000800000000)
  region size 2048K, 543 young (1112064K), 12 survivors (24576K)
 Metaspace       used 154163K, capacity 160620K, committed 160976K, reserved 1189888K
  class space    used 18071K, capacity 20476K, committed 20556K, reserved 1048576K
----

.prod third
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 2421454K [0x00000006f0000000, 0x0000000800000000)
  region size 2048K, 1117 young (2287616K), 12 survivors (24576K)
 Metaspace       used 154163K, capacity 160620K, committed 160976K, reserved 1189888K
  class space    used 18071K, capacity 20476K, committed 20556K, reserved 1048576K
----

The heap went from `925702 KB` to `2421454 KB`. When following the trend of the heap usage
lead can lead to the actual memory usage for this app (in the given cluster topology).


....
2.5 GB of used heap + 0.8 GB of non heap + 0.2 MB margin = 3.5 GB
....

Note this is the heap usage with the current GC activity, but this usage may eventually
be lower at the cost of higher GC activity and most likely CPU usage. This CPU usage
may require some adjustment on the deployment CPU limit, because on Kubernetes, if
a pod reached its CPU limit it gets throttled, and this would be very bad for a Java app
to be throttled.

The above number give the following percentage value `-XX:MaxRAMPercentage=71.0` with
a deployment limit of `3.5 GB`.



                         TODO XXXX


=== The lesson

The thing is that when this flag appeared (before it was `*RAMFraction`), almost only blogs (like this
https://merikan.com/2019/04/jvm-in-a-container/[one]) explored the options, thanks to them, but most are
incomplete to get the big picture, not to mention those who have slight errors.

The official documentation doesn't even mention `*RAMPercentage` flags:

.Oracle documentation
* https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE[`java` (JDK11)]
* https://docs.oracle.com/en/java/javase/12/docs/specs/man/java.html[`java` (JDK12)]

Fortunately there's still

{{< wrapTable >}}

.https://chriswhocodes.com/hotspot_options_jdk11.html[VM Options Explorer - JDK11 HotSpot]
|===
| Name             | Since | Deprecated | Type   | OS | CPU | Component | Default                   | Availability | Description                                                  | Defined in

| MaxRAMPercentage | JDK10 |            | double |    |     | gc        | 25.0 range(0.0, 100.0) | product      | Maximum percentage of real memory used for maximum heap size | `share/gc/shared/gc_globals.hpp`

|===

{{< /wrapTable >}}


Point taken, I already knew https://twitter.com/chriswhocodes[Chris Newland] useful websites
but didn't visit them to use this option, *I should have !*





== So as wrap-up

The memory of a process can be

====
* RSS => amount of physical memory allocated & used by a process
* Java MaxHeapSize != Docker stats (“MEM USAGE”)
** Java ~= heap + metaspace + off-heap (DirectBuffer + threads + compiled code + GC data + ...)
====




















=== Interpreting cgroup's memory

A good start is the actual Linux Kernel documentation on
https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt[cgroup v1].



.memory.stat
[source, bash]
----
❯ kubectl exec -it --container=java-app deployment/java-app -- cat /sys/fs/cgroup/memory/memory.stat
cache 57434112 <7>
rss 4822343680 <1>
rss_huge 0
shmem 0
mapped_file 0
dirty 0
writeback 0
swap 0 <6>
pgpgin 7918680
pgpgout 6726903
pgfault 7682598
pgmajfault 0
pgmajfault_s 0
pgmajfault_a 0
pgmajfault_f 0
inactive_anon 0 <2>
active_anon 4823887872 <3>
inactive_file 58806272 <4>
active_file 188416 <5>
unevictable 0
hierarchical_memory_limit 5368709120
hierarchical_memsw_limit 5368709120
total_cache 57434112
total_rss 4822343680
total_rss_huge 0
total_shmem 0
total_mapped_file 0
total_dirty 0
total_writeback 0
total_swap 0
total_pgpgin 7918680
total_pgpgout 6726903
total_pgfault 7682598
total_pgmajfault 0
total_pgmajfault_s 0
total_pgmajfault_a 0
total_pgmajfault_f 0
total_inactive_anon 0
total_active_anon 4823887872
total_inactive_file 58806272
total_active_file 188416
total_unevictable 0
----
<1> rss of the processes, anonymous memory and swap cache, without `tmpfs` (shmem) (~4.8 GB)
<2> anonymous memory and swap cache on active LRU list, with `tmpfs` (shmem)
<3> anonymous memory and swap cache on inactive LRU list, with `tmpfs` (shmem) (~4.8 GB)
<4> file-backed memory on inactive LRU list, in bytes (~59 MB)
<5> file-backed memory on active LRU list, in bytes (~190 KB)
<6> swap usage, `0` is the only good value for java
<7> page cache memory (~57 MB)

.From the https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-memory[RHEL6 documentation]
****
When you interpret the values reported by memory.stat, note how the various statistics inter-relate:

* `active_anon` + `inactive_anon` = anonymous memory + file cache for tmpfs + swap cache

Therefore, `active_anon` + `inactive_anon` ≠ rss, because rss does not include tmpfs.

* `active_file` + `inactive_file` = cache - size of tmpfs
****

There other memory settings to look at

.memory usage and limits
[source, bash]
----
cat /sys/fs/cgroup/memory/memory.{usage_in_bytes,limit_in_bytes,memsw.usage_in_bytes,memsw.limit_in_bytes}
4944756736 <1>
5368709120 <2>
4944748544 <3>
5368709120 <4>
----
<1> current memory usage ~4.9GB, but it's recommended to read cache+rss+swap values in `memory.stat`
<2> limit on the memory usage (~5.3GB)
<3> current memory and swap usage (~4.9 GB)
<4> limit on memory and swap (~5.3GB)

Note the `memory.limit_in_bytes` and `memory.memsw.limit_in_bytes` values are the same,
that means that the processes in the cgroup can use all the memory before swaping,
however it is not impossible for the process to be use the swap before this limit is reached.

In fact due to the swapiness value the kernel may try to reclaim memory.


There are other parameters related to the kernel and tcp allocations.

.memory.swapiness
[source, bash]
----
cat /proc/sys/vm/swappiness <1>
60
cat /sys/fs/cgroup/memory/memory.swappiness <2>
60
----
<1> OS swapiness
<2> cgroup swapiness, here the setting is not overridden


.log container details
[source]
----
-Xlog:pagesize,os,os+container:file=/gclogs/%t-os-container-pagezise.log:time,uptime,tags,level
----


.output
[source]
----
$ head -n 200 /gclogs/2020-05-22_22-28-32-os-container-pagezise.log
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] OSContainer::init: Initializing Container Support
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Path to /memory.use_hierarchy is /sys/fs/cgroup/memory/memory.use_hierarchy
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Use Hierarchy is: 1
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T22:28:33.405+0000][0.003s][info ][os,container] Memory Limit is: 5368709120
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu/cpu.cfs_quota_us
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] CPU Quota is: -1
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu/cpu.cfs_period_us
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] CPU Period is: 100000
[2020-05-22T22:28:33.405+0000][0.003s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu/cpu.shares
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] CPU Shares is: 1024
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] OSContainer::active_processor_count: 4
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu/cpu.cfs_quota_us
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] CPU Quota is: -1
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu/cpu.cfs_period_us
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] CPU Period is: 100000
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu/cpu.shares
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] CPU Shares is: 1024
[2020-05-22T22:28:33.406+0000][0.004s][trace][os,container] OSContainer::active_processor_count: 4
[2020-05-22T22:28:33.406+0000][0.004s][info ][os          ] Use of CLOCK_MONOTONIC is supported
[2020-05-22T22:28:33.406+0000][0.004s][info ][os          ] Use of pthread_condattr_setclock is supported
[2020-05-22T22:28:33.406+0000][0.004s][info ][os          ] Relative timed-wait using pthread_cond_timedwait is associated with CLOCK_MONOTONIC
[2020-05-22T22:28:33.406+0000][0.004s][info ][os          ] HotSpot is running with glibc 2.28, NPTL 2.28
[2020-05-22T22:28:33.406+0000][0.005s][info ][os          ] SafePoint Polling address, bad (protected) page:0x00007fa25e8ee000, good (unprotected) page:0x00007fa25e8ef000
[2020-05-22T22:28:33.406+0000][0.005s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so
[2020-05-22T22:28:33.407+0000][0.005s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so was successful
[2020-05-22T22:28:33.407+0000][0.005s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so
[2020-05-22T22:28:33.407+0000][0.005s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so was successful
[2020-05-22T22:28:33.408+0000][0.006s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so
[2020-05-22T22:28:33.408+0000][0.006s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libinstrument.so was successful
[2020-05-22T22:28:33.410+0000][0.008s][info ][os,thread   ] Thread attached (tid: 8, pthread id: 140335320479488).
[2020-05-22T22:28:33.410+0000][0.008s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libzip.so
[2020-05-22T22:28:33.410+0000][0.008s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libzip.so was successful
[2020-05-22T22:28:33.410+0000][0.008s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libjimage.so
[2020-05-22T22:28:33.410+0000][0.008s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libjimage.so was successful
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu/cpu.cfs_quota_us
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] CPU Quota is: -1
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu/cpu.cfs_period_us
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] CPU Period is: 100000
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu/cpu.shares
[2020-05-22T22:28:33.410+0000][0.008s][trace][os,container] CPU Shares is: 1024
[2020-05-22T22:28:33.410+0000][0.009s][trace][os,container] OSContainer::active_processor_count: 4
[2020-05-22T22:28:33.410+0000][0.009s][info ][pagesize    ] CodeHeap 'non-nmethods':  min=2496K max=5696K base=0x00007fa24090d000 page_size=4K size=5696K
[2020-05-22T22:28:33.410+0000][0.009s][info ][pagesize    ] CodeHeap 'profiled nmethods':  min=2496K max=120032K base=0x00007fa240e9d000 page_size=4K size=120032K
[2020-05-22T22:28:33.410+0000][0.009s][info ][pagesize    ] CodeHeap 'non-profiled nmethods':  min=2496K max=120032K base=0x00007fa2483d5000 page_size=4K size=120032K
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,cpu      ] CPU:total 4 (initial active 4) (1 cores per cpu, 1 threads per core) family 6 model 142 stepping 10, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, 3dnowpref, lzcnt, tsc, tscinvbit, bmi1, bmi2, fma
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,cpu      ] CPU Model and flags from /proc/cpuinfo:
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,cpu      ] model name  : Intel(R) Core(TM) i7-8559U CPU @ 2.70GHz
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,cpu      ] flags               : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht pbe syscall nx pdpe1gb lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq dtes64 ds_cpl ssse3 sdbg fma cx16 xtpr pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti fsgsbase bmi1 avx2 bmi2 erms xsaveopt arat
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,thread   ] Thread started (pthread id: 140335306258176, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.412+0000][0.010s][info ][os,thread   ] Thread is alive (tid: 9, pthread id: 140335306258176).
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Heap:  min=8M max=768M base=0x00000000d0000000 page_size=4K size=768M
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Block Offset Table: req_size=1536K base=0x00007fa240389000 page_size=4K alignment=4K size=1536K
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Card Table: req_size=1536K base=0x00007fa240209000 page_size=4K alignment=4K size=1536K
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Card Counts Table: req_size=1536K base=0x00007fa240089000 page_size=4K alignment=4K size=1536K
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Prev Bitmap: req_size=12M base=0x00007fa23b400000 page_size=4K alignment=4K size=12M
[2020-05-22T22:28:33.412+0000][0.011s][info ][pagesize    ] Next Bitmap: req_size=12M base=0x00007fa23a800000 page_size=4K alignment=4K size=12M
[2020-05-22T22:28:33.412+0000][0.011s][info ][os,thread   ] Thread started (pthread id: 140334742894336, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.413+0000][0.011s][info ][os,thread   ] Thread is alive (tid: 10, pthread id: 140334742894336).
[2020-05-22T22:28:33.413+0000][0.011s][info ][os,thread   ] Thread started (pthread id: 140334741837568, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.413+0000][0.011s][info ][os,thread   ] Thread is alive (tid: 11, pthread id: 140334741837568).
[2020-05-22T22:28:33.413+0000][0.012s][info ][os,thread   ] Thread started (pthread id: 140334703015680, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.413+0000][0.012s][info ][os,thread   ] Thread is alive (tid: 12, pthread id: 140334703015680).
[2020-05-22T22:28:33.413+0000][0.012s][info ][os,thread   ] Thread started (pthread id: 140334499624704, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.413+0000][0.012s][info ][os,thread   ] Thread is alive (tid: 13, pthread id: 140334499624704).
[2020-05-22T22:28:33.423+0000][0.021s][info ][os,thread   ] Thread started (pthread id: 140334490179328, attributes: stacksize: 1024k, guardsize: 4k, detached). 
[2020-05-22T22:28:33.423+0000][0.021s][info ][os,thread   ] Thread is alive (tid: 14, pthread id: 140334490179328).
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu/cpu.cfs_quota_us
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] CPU Quota is: -1
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu/cpu.cfs_period_us
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] CPU Period is: 100000
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu/cpu.shares
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] CPU Shares is: 1024
[2020-05-22T22:28:33.426+0000][0.024s][trace][os,container] OSContainer::active_processor_count: 4
[2020-05-22T22:28:33.427+0000][0.025s][info ][os,thread   ] Thread started (pthread id: 140334489122560, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.427+0000][0.025s][info ][os,thread   ] Thread is alive (tid: 15, pthread id: 140334489122560).
[2020-05-22T22:28:33.427+0000][0.025s][info ][os,thread   ] Thread started (pthread id: 140334488069888, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.427+0000][0.025s][info ][os,thread   ] Thread is alive (tid: 16, pthread id: 140334488069888).
[2020-05-22T22:28:33.435+0000][0.033s][info ][os,thread   ] Thread started (pthread id: 140334485497600, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.435+0000][0.033s][info ][os,thread   ] Thread is alive (tid: 17, pthread id: 140334485497600).
[2020-05-22T22:28:33.435+0000][0.033s][info ][os,thread   ] Thread started (pthread id: 140334484444928, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.435+0000][0.033s][info ][os,thread   ] Thread is alive (tid: 18, pthread id: 140334484444928).
[2020-05-22T22:28:33.435+0000][0.033s][info ][os,thread   ] Thread started (pthread id: 140334483392256, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.436+0000][0.034s][info ][os,thread   ] Thread is alive (tid: 19, pthread id: 140334483392256).
[2020-05-22T22:28:33.436+0000][0.034s][info ][os,thread   ] Thread started (pthread id: 140334482339584, attributes: stacksize: 1024k, guardsize: 0k, detached). 
[2020-05-22T22:28:33.437+0000][0.035s][info ][os,thread   ] Thread is alive (tid: 20, pthread id: 140334482339584).
[2020-05-22T22:28:33.438+0000][0.036s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T22:28:33.438+0000][0.036s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T22:28:33.438+0000][0.036s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T22:28:33.438+0000][0.036s][trace][os,container] Memory Usage is: 9551872
[2020-05-22T22:28:33.439+0000][0.037s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T22:28:33.439+0000][0.037s][trace][os,container] Memory Usage is: 9682944
...
[2020-05-22T22:28:33.460+0000][0.058s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T22:28:33.460+0000][0.058s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T22:28:33.460+0000][0.058s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T22:28:33.460+0000][0.058s][trace][os,container] Memory Usage is: 12820480
...
[2020-05-22T22:28:33.466+0000][0.064s][info ][os,thread   ] Thread started (pthread id: 140334481286912, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T22:28:33.466+0000][0.064s][info ][os,thread   ] Thread is alive (tid: 21, pthread id: 140334481286912).
...
[2020-05-22T15:40:29.807+0000][8005.132s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T15:40:29.807+0000][8005.132s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T15:40:29.807+0000][8005.132s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T15:40:29.807+0000][8005.132s][trace][os,container] Memory Usage is: 4093734912
[2020-05-22T15:40:38.913+0000][8014.238s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T15:40:38.914+0000][8014.238s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T15:40:38.914+0000][8014.238s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T15:40:38.914+0000][8014.238s][trace][os,container] Memory Usage is: 4093943808

----









(386453 - 1347 - 698) / 1028 => 373






////

https://pangin.pro/posts/stack-overflow-handling

https://stackoverflow.com/questions/25309748/what-is-thread-stack-size-option-xss-given-to-jvm-why-does-it-have-a-limit-of[What is thread stack size option(-Xss) given to jvm? Why does it have a limit of atleast 68k in a windows pc?]

Memory footprint of a Java process by Andrei Pangin
https://www.youtube.com/watch?v=c755fFv1Rnk

////