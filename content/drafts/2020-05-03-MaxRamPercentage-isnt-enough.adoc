---
authors: ["brice.dutheil"]
date: "2020-05-23T23:45:29+02:00"
language: en
#tags: ["cgroup", "java", "kubernetes", "docker", "memory", "jcmd", "heap", "pmap"]
slug: "maxrampercentage-isnt-what-i-wished-for"
title: "MaxRamPercentage is what I wished for"
draft: true
#_build:
#  list: never
---

Like many I was happy to see that the JDK landed support for containers
in Java 9, it was backported to Java 8 too.
Now JDK 11 does this by default and read information from `cgroups` and
it's possible to tell the memory usage via the `-XX:MaxRAMPercentage` flag.

However in practice this turned not not working as well as well as hoped.


.Problem
****
With a RAM percentage parameters at `85%` of `3 GiB` memory, the container
metrics showed the application actually went over this limit, which resulted
in killed pods and failing _deployment_ of our _replicaset_. The deployment
had was set up with an _horizontal pod auto-scaler_ too.

This issue started to appear with full traffic ; it was working well when
the traffic weight was 50%, it was even working well much `2 GiB` of heap
and 100% traffic. And from this point each of pods were oom killed.
****

image:/assets/maxrampercentage/pod-memory-usage.png[]
_The orange line is the pod memory limit, the increasing green line is the
total memory usage of the container `container_memory_usage_bytes`._

image:/assets/maxrampercentage/app-jvm-memory-usage.png[]
_The blue line is the heap limit `3 GiB`, the increasing green line
is the application RSS, yellow line is the heap usage._

This problem could be solved with a lot less information, it served as
an excuse to go down the rabbit hole of process memory exploration.
In doing so the following writing tries to gather various piece of information,
that are not easily available if you don't know what to look for or where,
gathering elements from a few things I knew, things I grepped in the
JDK codebase, and things learned from other (awesome) people.

I hope I didn't ignored important things, I hope I'm not totally wrong,
or simply I hope I'm not misguiding myself and especially others. If I'm wrong
please reach out.

Going back to trail.


[NOTE]
====
Memory figures use binary units (IEC), it matches the https://github.com/corretto/corretto-11/blob/055a9a1a279b9a2953c2150bc937b04f905eeba1/src/src/hotspot/share/utilities/globalDefinitions.hpp#L226[JVM],
our https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[Kubernetes] usage,
and Linux's tools (`/proc/{pid}/stat` or `/proc/{pid}/maps`, although I could find a reference).

Some graphs however show metric values instead of IEC, (1000 multiplier instead of 1024 (power of 2)).
====

== Lucky fixing the issue

The application that became problematic runs on a Kubernetes cluster. As mentioned above
this application worked fine before, and the people that handled the issue at that time were
not well-prepared, and I certainly wouldn't perform much better, so they tried memory limits
until it worked. `5 GiB` proved to be the lucky number.


.memory limits in the deployment object of the app
[source,yaml]
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: edge-api
  # ...
spec:
  # ...
  template:
    # ...
    spec:
      containers:
      - image: eu.gcr.io/cd-registry/java-app:1.20200513.102101-4c53e9c
        name: java-app
        # ...
        resources:
          limits:
            cpu: "8"
            memory: 5Gi # <1>
          requests:
            cpu: "3"
            memory: 3Gi
        # ...
----
<1> The working memory limit.

This works but this is not satisfactory :

1. This number is not even a guess, but a brute forced number.
2. This application worked with a `2 GiB` of heap, `5 GiB` looks greedy.
3. Again this number is not understood.


== Analyze

In order to understand our requirement, and to avoid lucky number (until it works),
let's explore the memory of a java process.


The RAM percentage are calculated at JVM startup from the given percentage, e.g.
with `-XX:InitialRAMPercentage=85.0` and `-XX:MaxRAMPercentage=85.0`) the JVM
calculates the following values.

.VM.flags in k8s
[source, bash]
----
$ jcmd $(pgrep java) VM.flags | tr ' ' '\n'
6:
-XX:CICompilerCount=4
-XX:ConcGCThreads=2
-XX:G1ConcRefinementThreads=8
-XX:G1HeapRegionSize=2097152
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=4563402752 <3>
-XX:InitialRAMPercentage=85.000000 <1>
-XX:+ManagementServer
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=4563402752 <4>
-XX:MaxNewSize=2736783360
-XX:MaxRAMPercentage=85.000000 <2>
-XX:MinHeapDeltaBytes=2097152
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5836300
-XX:NonProfiledCodeHeapSize=122910970
-XX:ProfiledCodeHeapSize=122910970
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC
----
<1> Initial RAM at 85%
<2> Max RAM at 85%
<3> Initial heap size ~`4.25 GiB`
<4> Max heap size ~`4.25 GiB`


=== Reading the memory footprint of the java process in the container

The first thing to look at is the _resident set size_, it can be obtained in
various way, e.g. using `ps` or reading the `/proc` should give the same number
if done at the same time.

.`ps`
[source, role="primary"]
----
$ ps o pid,rss -p $(pidof java)
PID   RSS
  6 4701120
----

.`/proc/{pid}/status`
[source, role="secondary"]
----
$ cat /proc/$(pgrep java)/status | grep VmRSS
VmRSS:	 4701120 kB
----

`4.6 GiB` !!! Not quite within the `4.25 GiB` (85% of `5 GiB`) limit. So let's dig a
bit to understand this number (4701120 KiB).

==== Digging in the java memory arenas

Fortunately the application started with `-XX:NativeMemoryTracking=summary` which
enables to have an overview of the different memory zones of a Java process.

NOTE: Enabling native memory tracking (NMT) causes a 5% to 10% performance overhead.

.`VM.native_memory` instant snapshot
[source, bash]
----
$ jcmd $(pgrep java) VM.native_memory scale=KB
6:

Native Memory Tracking:

Total: reserved=7168324KB, committed=5380868KB                               <1>
-                 Java Heap (reserved=4456448KB, committed=4456448KB)        <2>
                            (mmap: reserved=4456448KB, committed=4456448KB)

-                     Class (reserved=1195628KB, committed=165788KB)         <3>
                            (classes #28431)                                 <4>
                            (  instance classes #26792, array classes #1639)
                            (malloc=5740KB #87822)
                            (mmap: reserved=1189888KB, committed=160048KB)
                            (  Metadata:   )
                            (    reserved=141312KB, committed=139876KB)
                            (    used=135945KB)
                            (    free=3931KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=20172KB)
                            (    used=17864KB)
                            (    free=2308KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=696395KB, committed=85455KB)
                            (thread #674)
                            (stack: reserved=692812KB, committed=81872KB)    <5>
                            (malloc=2432KB #4046)
                            (arena=1150KB #1347)

-                      Code (reserved=251877KB, committed=105201KB)          <6>
                            (malloc=4189KB #11718)
                            (mmap: reserved=247688KB, committed=101012KB)

-                        GC (reserved=230739KB, committed=230739KB)          <7>
                            (malloc=32031KB #63631)
                            (mmap: reserved=198708KB, committed=198708KB)

-                  Compiler (reserved=5914KB, committed=5914KB)              <8>
                            (malloc=6143KB #3281)
                            (arena=18014398509481755KB #5)

-                  Internal (reserved=24460KB, committed=24460KB)           <10>
                            (malloc=24460KB #13140)

-                     Other (reserved=267034KB, committed=267034KB)         <11>
                            (malloc=267034KB #631)

-                    Symbol (reserved=28915KB, committed=28915KB)            <9>
                            (malloc=25423KB #330973)
                            (arena=3492KB #1)

-    Native Memory Tracking (reserved=8433KB, committed=8433KB)
                            (malloc=117KB #1498)
                            (tracking overhead=8316KB)

-               Arena Chunk (reserved=217KB, committed=217KB)
                            (malloc=217KB)

-                   Logging (reserved=7KB, committed=7KB)
                            (malloc=7KB #266)

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #521)

-                    Module (reserved=1362KB, committed=1362KB)
                            (malloc=1362KB #6320)

-              Synchronizer (reserved=837KB, committed=837KB)
                            (malloc=837KB #6877)

-                 Safepoint (reserved=8KB, committed=8KB)
                            (mmap: reserved=8KB, committed=8KB)

-                   Unknown (reserved=32KB, committed=32KB)
                            (mmap: reserved=32KB, committed=32KB)
----
<1> This shows a `reserved` value (`7168324 KiB` (~`6.84 GiB`)), it's the amount of addressable memory
(all OS types) on that container, and a `committed` value (`4456448 KiB` (~`4.25 GiB`)) that represents
what the JVM actually told the OS to allocate.
<2> Heap arena, note reserved and committed values are the same `4456448 KiB` here because our
`InitialRAMPercentage` is the same as max. I'm not sure why this number is different from the VM
flags `-XX:MaxHeapSize=4563402752` though.
<3> ~`162 MiB` of metaspace
<4> How many classes have been loaded : `28431`
<5> There are 674 threads that are using ~`80 MiB` and could use up to ~`676 MiB`.
<6> Code cache area (assembly of the used methods) ~`102 MiB` out of ~`246 MiB` which matches with `-XX:ReservedCodeCacheSize=251658240`
<7> G1GC internal data structures take ~`225 MiB`
<8> C1 / C2 compilers (which compile bytecodes to assembly) uses ~`5.8 MiB`
<9> The symbols contains many things like interned strings and other internal constants ~`28.2 MiB`
<10> Internal (includes `DirectByteBuffers` before Java 11), maybe others objects, here takes ~`24 MiB`
<11> Other section after Java 11 includes `DirectByteBuffers` ~`261 MiB`

Other areas are much smaller in scale, NMT takes ~`8.2 MiB` itself, module system usage ~`1.3 MiB`,
etc. Also, note that enabling other JVM features may show up if they are activated.
https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-5EF7BB07-C903-4EBD-A9C2-EC0E44048D37[Source]


[NOTE]
====
At the time this report was executed the committed memory is `5380868 KiB` (`5.13 GiB`) while
the process RSS is `4701120 KiB`. The difference relates to how `mmap` works (on Linux), memory
pages are only backed by physical memory once they're written to.

Some people may have heard of the `-XX:+AlwaysPreTouch` Hotspot option. This option tells
the JVM to always write zeroes to memory pages (which has the effect of reducing physical memory
commit latencies), but this option only affect the heap memory allocations. Other zones like
thread stack or metaspace work differently, that means some *committed* memory shown in NMT is not
*resident* and not accounted by the RSS counter.

Here's some memory related vocabulary that help distinguish between memory _statuses_ :

.vocabulary breakdown
[%autowidth.stretch]
|===

| *Used Heap* | The amount of memory occupied by live objects according.

| *Committed* | Address ranges that have been mapped with something other than `PROT_NONE`.
They may or may not be backed by physical or swap due to lazy allocation and paging.

| *Reserved* | The total address range that has been pre-mapped via `mmap` for a particular memory pool.
The _reserved_ / _committed_ difference consists of `PROT_NONE` mappings, which are guaranteed to not be backed
by physical memory

| *Resident* | Pages which are currently in physical ram. This means code, stacks, part of the committed memory
pools but also portions of ``mmap``ed files which have recently been accessed and allocations outside the control
of the JVM.

| *Virtual* | The sum of all virtual address mappings. Covers committed, reserved memory pools but also mapped
files or shared memory. This number is rarely informative since the JVM can reserve very large address
ranges in advance or mmap large files.

|===

https://stackoverflow.com/a/31178912/48136[source]
====

There's a lot more to read on the
https://docs.oracle.com/en/java/javase/11/vm/native-memory-tracking.html#GUID-39676837-DA61-4F8D-9C5B-9DB1F5147D80[official documentation about NMT]
and https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-1F53A50E-86FF-491D-A023-8EC4F1D1AC77[how to Monitor VM Internal Memory].

And another worthwhile read on https://shipilev.net/jvm/anatomy-quarks/12-native-memory-tracking/[native memory tracking]
by http://twitter.com/shipilev[Aleksey Shipilёv].


==== Explore what NMT does not show

There's also the `MappedByteBuffers`, these are the files mapped to virtual memory of a process.
NMT does not track them, however, `MappedByteBuffers` can also take physical memory. It's possible
to see the actual usage of a process memory map: `pmap -x <pid>`


.process memory mappings
[source, bash]
----
$ pmap -x $(pgrep java)
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
-XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
-Xlog:os,safepoint*,gc*,gc+ref=debug,gc+ergo*=debug,gc+age*=debug,gc+phases*:file=/gclogs/%t-gc.log:time,uptime,tags:filecount=5,filesize=10M -javaag
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
000000000216f000     404     272     272 rw---   [ anon ]
00000006f0000000 4476620 3128252 3128252 rw---   [ anon ]
00000008013b3000 1028404       0       0 -----   [ anon ]
00007fc5de9ea000      16       0       0 -----   [ anon ]
00007fc5de9ee000    1012     104     104 rw---   [ anon ]
00007fc5deaeb000      16       0       0 -----   [ anon ]
00007fc5deaef000    1012      24      24 rw---   [ anon ]
00007fc5debec000      16       0       0 -----   [ anon ]
00007fc5debf0000    1012      92      92 rw---   [ anon ]
00007fc5deced000      16       0       0 -----   [ anon ]
00007fc5decf1000    1012     100     100 rw---   [ anon ]
00007fc5dedee000      16       0       0 -----   [ anon ]
00007fc5dedf2000    1012     100     100 rw---   [ anon ]
00007fc5deeef000      16       0       0 -----   [ anon ]
00007fc5deef3000    1012     100     100 rw---   [ anon ]
00007fc5deff0000      16       0       0 -----   [ anon ]
00007fc5deff4000    1012     100     100 rw---   [ anon ]
00007fc5df0f1000      16       0       0 -----   [ anon ]
00007fc5df0f5000    1012     100     100 rw---   [ anon ]
00007fc5df1f2000      16       0       0 -----   [ anon ]
00007fc5df1f6000    1012     100     100 rw---   [ anon ]
00007fc5df2f3000      16       0       0 -----   [ anon ]
00007fc5df2f7000    1012     100     100 rw---   [ anon ]
00007fc5df3f4000      16       0       0 -----   [ anon ]
00007fc5df3f8000    1012     100     100 rw---   [ anon ]
00007fc5df4f5000      16       0       0 -----   [ anon ]
00007fc5df4f9000    1012     100     100 rw---   [ anon ]
00007fc5df5f6000      16       0       0 -----   [ anon ]
00007fc5df5fa000    1012     100     100 rw---   [ anon ]

...

00007fca48ba9000   17696   14876       0 r-x-- libjvm.so
00007fca49cf1000    2044       0       0 ----- libjvm.so
00007fca49ef0000     764     764     764 r---- libjvm.so
00007fca49faf000     232     232     208 rw--- libjvm.so
00007fca49fe9000     352     320     320 rw---   [ anon ]
00007fca4a041000     136     136       0 r---- libc-2.28.so
00007fca4a063000    1312    1140       0 r-x-- libc-2.28.so
00007fca4a1ab000     304     148       0 r---- libc-2.28.so
00007fca4a1f7000       4       0       0 ----- libc-2.28.so
00007fca4a1f8000      16      16      16 r---- libc-2.28.so
00007fca4a1fc000       8       8       8 rw--- libc-2.28.so
00007fca4a1fe000      16      16      16 rw---   [ anon ]
00007fca4a202000       4       4       0 r---- libdl-2.28.so
00007fca4a203000       4       4       0 r-x-- libdl-2.28.so
00007fca4a204000       4       4       0 r---- libdl-2.28.so
00007fca4a205000       4       4       4 r---- libdl-2.28.so
00007fca4a206000       4       4       4 rw--- libdl-2.28.so
00007fca4a207000     100     100       0 r-x-- libjli.so
00007fca4a220000    2048       0       0 ----- libjli.so
00007fca4a420000       4       4       4 r---- libjli.so
00007fca4a421000       4       4       4 rw--- libjli.so
00007fca4a422000      24      24       0 r---- libpthread-2.28.so
00007fca4a428000      60      60       0 r-x-- libpthread-2.28.so
00007fca4a437000      24       0       0 r---- libpthread-2.28.so
00007fca4a43d000       4       4       4 r---- libpthread-2.28.so
00007fca4a43e000       4       4       4 rw--- libpthread-2.28.so
00007fca4a43f000      16       4       4 rw---   [ anon ]
00007fca4a443000       4       4       0 r---- LC_IDENTIFICATION
00007fca4a444000       4       0       0 -----   [ anon ]
00007fca4a445000       4       0       0 r----   [ anon ]
00007fca4a446000       8       8       8 rw---   [ anon ]
00007fca4a448000       4       4       0 r---- ld-2.28.so
00007fca4a449000     120     120       0 r-x-- ld-2.28.so
00007fca4a467000      32      32       0 r---- ld-2.28.so
00007fca4a46f000       4       4       4 r---- ld-2.28.so
00007fca4a470000       4       4       4 rw--- ld-2.28.so
00007fca4a471000       4       4       4 rw---   [ anon ]
00007ffe28536000     140      40      40 rw---   [ stack ]
00007ffe28582000      12       0       0 r----   [ anon ]
00007ffe28585000       8       4       0 r-x--   [ anon ]
ffffffffff600000       4       0       0 r-x--   [ anon ]
---------------- ------- ------- -------
total kB         24035820 4776860 4720796
----

Let's refine that with more
https://www.kernel.org/doc/Documentation/filesystems/proc.txt[knowledge about `/proc/{pid}/maps`],
it indicates that a _map_ has a set of modes:

* `r-`: readable memory mapping
* `w`: writable memory mapping
* `x`: executable memory mapping
* `s` or `p` : shared memory mapping or private mapping. `/proc/<pid>/maps` shows both
but `pmap` only show the `s` flag.

On a side note, `pmap` may show another mapping mode which I barely found any reference of,
here's https://johanlouwers.blogspot.com/2017/07/oracle-linux-understanding-linux.html[one]
and https://linux.die.net/man/2/mmap[here]

* `R`: if set, the map has no swap space reserved (`MAP_NORESERVE` flag of `mmap`).
This means that we can get a segmentation fault by accessing that memory if it has not
already been mapped to physical memory, and if the system is out of physical memory.

At this time the focus is to see what are the memory mapped files with the JVM. Those can be either
read from or written to, we need to look for both the `r` or `w` or neither, also while quite unlikely
with Java let's not restrict on the _executable_ mapping, so the only thing we could be restricting to
is the shared mapping `s` (memory mapped files are shared because the OS may want to reuse the afferent
memory pages for other processes) :

.Our application memory mapped files
[source, bash]
----
$ pmap -x 6 | grep "[r-][w-][x-][s][R-]"
00007f5fdc02f000       4       4       0 r--s- instrumentation1647616515145161084.jar
00007f5fdc030000       4       4       0 r--s- instrumentation11262564974060761935.jar
00007f5fdc053000       8       8       0 r--s- java-agent-bs-cl.jar
00007f5fdc055000       4       4       0 r--s- instrumentation249633448216144460.jar
00007f5fdc056000       4       4       0 r--s- agent1-bootstrap10447345921091566771.jar
00007f5fdc057000      12      12       0 r--s- agent1-api6038277081136135384.jar
00007f5fec000000       8       8       0 r--s- agent1-weaver-api16247655721253674284.jar
00007f5fec002000       4       4       0 r--s- agent1-opentracing-bridge12060425782296980104.jar
00007f5fec003000      12      12       0 r--s- agent2-bridge3261511391751138774.jar
00007f5ffb910000  138176   36060       0 r--s- modules
00007f6008006000      28      28       0 r--s- gconv-modules.cache
                           ^^^^^               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
----

There's around `35.2 MiB` of memory mapped files.

_As I was a bit unfamiliar with `pmap`, reading https://techtalk.intersec.com/2013/07/memory-part-2-understanding-process-memory/[this process memory blog]
helped me with the above command._

Wrapping this information from NMT and memory mapped files leaves us with the
following _equation_ to estimate the actual memory usage of a process:

....
Total memory = Heap + GC + Metaspace + Code Cache + Symbol tables
               + Compiler + Other JVM structures + Thread stacks
               + Direct buffers + Mapped files +
               + Native Libraries + Malloc overhead + ...
....

[%autowidth.stretch,options="footer"]
|===

| Heap                            | 4456448
| GC                              |  230739
| Metaspace                       |  165788
| Code Cache                      |  105201
| Symbol tables                   |   28915
| Compiler                        |    5914
| Other JVM structures
(Internal + NMT + smaller area)   |   24460 + 8433 + 217 + 7 + 19 + 1362 + 837 + 8 + 32
| Thread stacks                   |   85455
| Direct buffers (Other)          |  267034
| Mapped files                    |   36060 + 4 + 4 + 8 + 4 + 4 + 12 + 8 + 4 + 12 + 28
| Native Libraries                | unaccounted at this time
| Malloc overhead                 | accounted in NMT
| ...                             |

| Total                           | 5242880 KiB
|===

`5186278 KiB` is what this container is actually using, so way over the RSS (`4701120 KiB`)
but also over the `5 GiB` (`5242880 KiB`) of the pod limit. Yet this pod is healthy and far from
the thresholds to be oom killed.

*So what I am missing here ?*

There a few considerations to understand :

* NMT shows _reserved_ and _committed_ values on each arenas,
+
[%autowidth.stretch]
|===

| `reserved` | this is the size that the OS guarantees to be available (but the
JVM didn't tell the OS to allocate this memory)
| `committed` | this size indicate the memory that the JVM allocated on the OS

|===
+
Each of these memory arenas are managed differently: `GC`, `Compiler` have the
same committed and reserved memory values while other zones have the ability to
shrink or grow for example `thread stacks` arenas reports
`85455 KiB` but could take up to `696395 KiB` if necessary, and theoretically
same as the heap.

* While the JVM did allocate this memory, Linux on x86 hardware uses virtual
memory with paging. More specifically Linux optimize actual physical memory
and only commit a page physically if this page is actually written to. In this
case the `Heap` arena in particular seems to benefit from this behavior as the JVM
allocated `4456448 KiB`, but the actual RAM _resident set size_ usage of this memory
zone seems at this time is `3128252 KiB`.

Where to look this number? While it's easy to get the RSS of a process, to understand
if the committed heap actually _resides_ on physical memory you need to use `pmap` or
inspect `/proc/{pid}/maps` or `/proc/{pid}/smaps`. You have to notice the one of the first
memory zone is quite big and about the size of the committed heap as shown in NMT. It's easier
to spot with `pmap -X` (capital `X`). _Note the below capture are from a different pod/process_.

.`pmap -x <pid>`
[source, role="primary"]
----
$ pmap -x $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
0000000001cfc000     412     224     224 rw---   [ anon ]
00000006f0000000 4477472 2944744 2944744 rw---   [ anon ] <1>
0000000801488000 1027552       0       0 -----   [ anon ]
00007f11b3744000   16388   16388   16388 rw---   [ anon ]
00007f11b4745000      16       0       0 -----   [ anon ]
00007f11b4749000   50688   49484   49484 rw---   [ anon ]
00007f11b78c9000    1536       0       0 -----   [ anon ]
00007f11b7a49000   32776   32776   32776 rw---   [ anon ]
00007f11b9a4b000      16       0       0 -----   [ anon ] <2>
00007f11b9a4f000    1012      24      24 rw---   [ anon ] <3>
00007f11b9b4c000      16       0       0 -----   [ anon ]
00007f11b9b50000    1012      92      92 rw---   [ anon ]
00007f11b9c4d000      16       0       0 -----   [ anon ]
00007f11b9c51000    1012     116     116 rw---   [ anon ]
...
----
<1> heap arena
<2> a thread guard pages
<3> a thread stack

.`pmap- X <pid>`
[source, role="secondary"]
----
$ pmap -X $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom -XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
         Address Perm   Offset Device   Inode     Size     Rss     Pss Referenced Anonymous LazyFree ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible Mapping
        00400000 r-xp 00000000  08:01 4054960        4       4       1          4         0        0              0              0               0    0       0      0           0 java
        00600000 r--p 00000000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        00601000 rw-p 00001000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        01cfc000 rw-p 00000000  00:00       0      412     224     224        224       224        0              0              0               0    0       0      0           0 [heap] <1>
       6f0000000 rw-p 00000000  00:00       0  4477472 2939592 2939592    2939592   2939592        0              0              0               0    0       0      0           0
       801488000 ---p 00000000  00:00       0  1027552       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4745000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4749000 rw-p 00000000  00:00       0    50688   49472   49472      49472     49472        0              0              0               0    0       0      0           0
    7f11b78c9000 ---p 00000000  00:00       0     1536       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b7a49000 rw-p 00000000  00:00       0    32776   32776   32776      32776     32776        0              0              0               0    0       0      0           0
    7f11b9a4b000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0        <2>
    7f11b9a4f000 rw-p 00000000  00:00       0     1012     112     112        112       112        0              0              0               0    0       0      0           0        <3>
    7f11b9b4c000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9b50000 rw-p 00000000  00:00       0     1012      96      96         96        96        0              0              0               0    0       0      0           0
    7f11b9c4d000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9c51000 rw-p 00000000  00:00       0     1012     116     116        116       116        0              0              0               0    0       0      0           0
...
----
<1> heap arena
<2> a thread guard pages
<3> a thread stack


== Going back to choose a better value for the RAM percentage

From the above, it's now possible with NMT especially and with `pmap` to
understand actual memory usage and to answer the question: "What is a sensible
RAM percentage setting for this application ?"

Really what drive the answer is the actual non-heap usage not accounted in
`MaxRAMPercentage`, from the numbers above:

....
(total) 5242880 - (heap) 4456448 = 786432 KiB
....


.In percentages
[%autowidth.stretch,options="footer"]
|===

| Non heap | 5242880 - 4456448 = 786432 | ~14 %
| Heap     | 4456448                    | ~86 %

| Total    | 5186278                    | 100 %
|===

*This means the application needs around `790 MiB`, plus the heap to run.*

From the flags seen above, the JVM set the heap maximum size memory to `4 563 402 752` Bytes,
this value was computed from this flag `-XX:MaxRAMPercentage=85.000000`, and this percentage
is somehow a lucky guess that worked for the `5 GiB` deployment memory limit.
But this actual percentage is in fact _wrong_, if he JVM needed all the memory within the max
heap plus bigger stack traces then the container/pod would have been _oom killed_. Also, it is
necessary to give some free space in the container
to be able to perform serviceability tasks, like profiling, heap dump, etc.

For a `5 GiB` limit it may be good to give around 20% for all of these non-heap, plus system space
for this particular workload (e.g. if the application requires heavy filesystem usage, then
it would be a different number to make room for the filesystem cache).

So the problem would be solved with the following value, for a `5 GiB` memory limit :

[source]
----
-XX:InitialRAMPercentage=80.0 <1>
-XX:MaxRAMPercentage=80.0 <1>
----



For a quick win let's adapt the application image.

== Make the docker image memory settings tweakable per environment

AS seen at beginning of this post, RAM settings are part of the command declaration, this
is not suitable as seen above. In addition, the deployment requirements / limits is likely to
differ depending on the cluster / environment. One good reason would be to decrease the money spending
on your cloud provider for non-production clusters, like staging, pre-production, etc.
It will be useful to enable flexibility one setting the application for any given environment.

Let's use https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-0A40ECEE-AFDF-48CB-AF7C-A33DDE07A8DC[`JAVA_TOOL_OPTIONS`]
environment variable to enable flexibility and remove the RAM percentage in the `CMD` directive.

.Application dockerfile
[source,diff]
----
  ARG REGISTRY
  FROM $REGISTRY/corretto-java:11.0.6.10.1
+ ENV JAVA_TOOL_OPTIONS="" <1>

  RUN mkdir -p /gclogs /etc/java-app

  COPY ./build/libs/java-app-boot.jar \
    ./build/java-agents/agent-1.jar \
    ./build/java-agents/agent-2.jar \
    ./src/serviceability/*.sh \
    /

  CMD [ "/usr/bin/java", \
        "-Dfile.encoding=UTF-8", \
        "-Duser.timezone=UTC", \
        "-Dcom.sun.management.jmxremote.port=7199", \
        "-Dcom.sun.management.jmxremote.rmi.port=7199", \
        "-Dcom.sun.management.jmxremote.ssl=false", \
        "-Dcom.sun.management.jmxremote.authenticate=false", \
        "-Djava.security.egd=file:/dev/./urandom", \
-       "-XX:InitialRAMPercentage=85.0", \ <2>
-       "-XX:MaxRAMPercentage=85.0", \
        "-XX:NativeMemoryTracking=summary", \
        "-Xlog:os,safepoint*,gc*,gc+ref=debug,gc+ergo*=debug,gc+age*=debug,gc+phases*:file=/gclogs/%t-gc.log:time,uptime,tags:filecount=5,filesize=10M", \
        "-javaagent:/agent-1.jar", \
        "-javaagent:/agent-2.jar", \
        "-Dsqreen.config_file=/sqreen.properties", \
        "-jar", \
        "/java-app-boot.jar", \
        "--spring.config.additional-location=/etc/java-app/config.yaml", \
        "--server.port=8080" ]

  LABEL name="java-app"
  LABEL build_path="../"
  LABEL version_auto_semver="true"
----
<1> Defines the https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-0A40ECEE-AFDF-48CB-AF7C-A33DDE07A8DC[`JAVA_TOOL_OPTIONS`]
<2> Removes the RAM percentage settings to get _default_ values.

Now let's test this locally to play a bit.

.Build the container
[source]
----
❯ DOCKER_BUILDKIT=1 docker build \
  --tag test-java-app \ <1>
  --build-arg REGISTRY=eu.gcr.io/cd-registry \
  --file _infra/Dockerfile \
  .
[+] Building 1.4s (9/9) FINISHED
 => [internal] load build definition from Dockerfile                                                                                              0.0s
 => => transferring dockerfile: 1.34kB                                                                                                            0.0s
 => [internal] load .dockerignore                                                                                                                 0.0s
 => => transferring context: 35B                                                                                                                  0.0s
 => [internal] load metadata for eu.gcr.io/cd-registry/corretto-java:11.0.6.10.1                                                                  0.0s
 => CACHED [1/4] FROM eu.gcr.io/cd-registry/corretto-java:11.0.6.10.1                                                                             0.0s
 => [internal] load build context                                                                                                                 0.0s
 => => transferring context: 1.32kB                                                                                                               0.0s
 => [2/4] RUN mkdir -p /gclogs /etc/java-app                                                                                                      0.3s
 => [3/4] COPY ./build/async-profiler/linux-x64 /async-profiler                                                                                   0.0s
 => [4/4] COPY ./build/libs/java-app-boot.jar   ./build/java-agents/agent-1.jar   ./build/java-agents/agent-2.jar   ./src/serviceability/*.sh   / 0.6s
 => exporting to image                                                                                                                            0.4s
 => => exporting layers                                                                                                                           0.4s
 => => writing image sha256:5ceef8f5a4e23cb3bea7ca7cb7c90c0e338386b7f37992c92861cb119c312cb9                                                      0.0s
 => => naming to docker.io/library/test-java-app
----
<1> Custom tag to avoid collision with regular images in my cache

Run the container with the Java app

.*Without* `JAVA_TOOL_OPTIONS`
[source,role="primary"]
----
❯ docker run --rm --memory="3gb" --name j-mem test-java-app
Picked up JAVA_TOOL_OPTIONS:
10:14:53.566 [main] INFO org.springframework.core.KotlinDetector - Kotlin reflection implementation not found at runtime, related features won't be available.
2020-03-20 10:14:55.616 [] WARN  --- [kground-preinit] o.s.h.c.j.Jackson2ObjectMapperBuilder    : For Jackson Kotlin classes support please add "com.fasterxml.jackson.module:jackson-module-kotlin" to the classpath
...
----

.*With* `JAVA_TOOL_OPTIONS`
[source,role="secondary"]
----
❯ docker run --rm --memory="3gb" --env JAVA_TOOL_OPTIONS="-XX:InitialRAMPercentage=70.0 -XX:MaxRAMPercentage=70.0" --name j-mem test-java-app
Picked up JAVA_TOOL_OPTIONS: -XX:InitialRAMPercentage=70.0 -XX:MaxRAMPercentage=70.0
10:14:53.566 [main] INFO org.springframework.core.KotlinDetector - Kotlin reflection implementation not found at runtime, related features won't be available.
2020-03-20 10:14:55.616 [] WARN  --- [kground-preinit] o.s.h.c.j.Jackson2ObjectMapperBuilder    : For Jackson Kotlin classes support please add "com.fasterxml.jackson.module:jackson-module-kotlin" to the classpath
...
----


Then we can make sure we have the correct flags.

.*Without* `JAVA_TOOL_OPTIONS`
[source, role="primary"]
----
❯ docker exec -it j-mem bash -c "jcmd \$(pgrep java) VM.flags | tr ' ' '\n'"
6:
-XX:CICompilerCount=3
-XX:ConcGCThreads=1
-XX:G1ConcRefinementThreads=4
-XX:G1HeapRegionSize=1048576
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=50331648
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=805306368 <1>
-XX:MaxNewSize=482344960
-XX:MinHeapDeltaBytes=1048576
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5830732
-XX:NonProfiledCodeHeapSize=122913754
-XX:ProfiledCodeHeapSize=122913754
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC

----
<1> Max heap is about `768 MiB`

.*With* `JAVA_TOOL_OPTIONS`
[source, role="secondary"]
----
❯ docker exec -it j-mem bash -c "jcmd \$(pgrep java) VM.flags | tr ' ' '\n'"
6:
-XX:CICompilerCount=3
-XX:ConcGCThreads=1
-XX:G1ConcRefinementThreads=4
-XX:G1HeapRegionSize=1048576
-XX:GCDrainStackTargetSize=64
-XX:InitialHeapSize=2256535552
-XX:InitialRAMPercentage=70.000000
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=2256535552 <1>
-XX:MaxNewSize=1353711616
-XX:MaxRAMPercentage=70.000000
-XX:MinHeapDeltaBytes=1048576
-XX:NativeMemoryTracking=summary
-XX:NonNMethodCodeHeapSize=5830732
-XX:NonProfiledCodeHeapSize=122913754
-XX:ProfiledCodeHeapSize=122913754
-XX:ReservedCodeCacheSize=251658240
-XX:+SegmentedCodeCache
-XX:+UseCompressedClassPointers
-XX:+UseCompressedOops
-XX:+UseFastUnorderedTimeStamps
-XX:+UseG1GC

----
<1> Max heap is about `2.1 GiB`


Notice when there's no RAM settings the JVM computed the max heap size at 25%
of memory constraints `3 GiB`. And to 80%, `2.1 GiB`, of the same limit when
passing the RAM percentages. Also, the heap values are the only one affected,
other memory areas default values kept the same values.


== Going further

As a reminder this application was set up with 85% max heap when the
deployment limit was `3 GiB`, it worked well under 50% of the traffic but failed with full traffic.
Then this pod memory limit was bumped to `5 GiB` and the pod wasn't anymore oomkilled.
How this _limit_ was found is a lucky guess, given the RAM percentages were set in the `CMD`
directive of the Dockerfile.

As identified above there are two, maybe three arenas whose usage may explain the surge in memory before
the memory limit was increased. I don't have anything to back that except how I expect these memory arenas
to grow but not the others.

1. The thread stack memory arena, the increase actual memory pages is small, but enough to be mentioned.

2. The GC internal memory arena, with more threads there are more allocations, and as such more
things to track.

3. The _other_ memory arenas with more `DirectByteBuffers` usage.

4. And anyway with more thread there could be a bit more allocations, which means more
memory pages needed.

image:/assets/maxrampercentage/app-file-descriptors.png[]

The heap had a max value anyway, and if it was then the app would either trigger full gcs, or self terminated
with an `OutOfMemoryError`, so that not the heap. AS for the offers it's unlikely with the workload they grow
that much.

My hypothesis is that when full traffic came to this pod, these arenas grew by `100 MiB` to `200 MiB` (sum),
while not much, it was sufficient to go over the 15% of memory left for the non heap memory, and thus triggered
the system oom killer.


Also, at some point in time this application worked well under way less memory in a different cluster `-Xmx=2g`.
The code is not the culprit in this case. Let's explore that.

=== Actual Java Heap usage

While the previous section allowed to understand the actual memory usage, it didn't give any figure
regarding the actual heap usage for this application :

.GC.heap_info
[source, role="primary"]
----
$ jcmd $(pgrep java) GC.heap_info
6:
 garbage-first heap   total 4456448K, used 537569K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 161 young (329728K), 13 survivors (26624K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`525 MiB`

.1
[source, role="secondary"]
----
$ jcmd $(pgrep java) GC.heap_info
6:
 garbage-first heap   total 4456448K, used 925702K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 387 young (792576K), 12 survivors (24576K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`904 MiB`

.2
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 1245902K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 543 young (1112064K), 12 survivors (24576K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`1,217 MiB`

.3
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 2421454K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 1117 young (2287616K), 12 survivors (24576K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`2,364 MiB`

.4
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 2715248K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 1225 young (2508800K), 13 survivors (26624K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`2,652 MiB`

.5
[source, role="secondary"]
----
$ jcmd 6 GC.heap_info
6:
 garbage-first heap   total 4456448K, used 279521K [0x00000006f0000000, 0x0000000800000000) <1>
  region size 2048K, 35 young (71680K), 13 survivors (26624K)
 Metaspace       used 154131K, capacity 160610K, committed 160976K, reserved 1189888K
  class space    used 18070K, capacity 20474K, committed 20556K, reserved 1048576K
----
<1> Used heap : ~`273 MiB`

On the application in production, limited with `5 GiB` of memory, the heap
seems to increase between something like `273 MiB` to `2.7 GiB`. Graphing the trend of the heap usage
over time suggests the memory usage for this app (for the current cluster topology
(_replicaset_, traffic, etc.)).

image::/assets/maxrampercentage/app-heap-usage-with-5GiB-limit-85p-max.png[]

To keep things simple let's use the rough top usage of `2.7 GiB` of the heap. While the available
allocated heap is `4.25 GiB`. As a reminder non-used memory pages are not physically in RAM,
thanks to the OS (in that case Linux), look at the `RSS` column of the `pmap` output.

So just using this heap usage with the non heap usage, plus some margin, gives this number :

....
2.7 GiB of used heap + 0.8 GiB of non heap + 0.2 GiB margin = 3.7 GiB
....

Again keep in mind this is the heap usage with the current GC activity. As said earlier
this application worked with a lower heap `2 GiB`, this certainly worked at the cost of
higher GC activity and CPU usage at that time, this is ok as this workload is mostly IO bound.
But with restraints in Kubernetes care must be taken otherwise the pod may be throttled.

Anyway this CPU usage may require some adjustment on the deployment CPU limit
(https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[millicores]).
This is essential because on Kubernetes, if a pod reached its CPU limit it gets
https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits?hl=fa[throttled],
and this very bad for a Java app to be throttled (this is the same for a Go application).

Going back to our _equation_ above, those numbers yield the following percentage
`-XX:MaxRAMPercentage=72.97` for a deployment limit of `3.7 GiB`.


 _____ ___  ____   ___
|_   _/ _ \|  _ \ / _ \
  | || | | | | | | | | |
  | || |_| | |_| | |_| |
  |_| \___/|____/ \___/

                         TODO VALIDATE in prod
                         TODO Use -Xmx=2g





=== The lesson

The thing is that when this flag appeared (before it was `*RAMFraction`), almost only blogs (like this
https://merikan.com/2019/04/jvm-in-a-container/[one]) explored the options, thanks to them, but most are
incomplete to get the big picture, not to mention those who have slight errors.

The official documentation doesn't even mention `*RAMPercentage` flags:

.Oracle documentation
* https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE[`java` (JDK11)]
* https://docs.oracle.com/en/java/javase/12/docs/specs/man/java.html[`java` (JDK12)]

Fortunately there's still

{{< wrapTable >}}

.https://chriswhocodes.com/hotspot_options_jdk11.html[VM Options Explorer - JDK11 HotSpot]
|===
| Name             | Since | Deprecated | Type   | OS | CPU | Component | Default                   | Availability | Description                                                  | Defined in

| MaxRAMPercentage | JDK10 |            | double |    |     | gc        | 25.0 range(0.0, 100.0) | product      | Maximum percentage of real memory used for maximum heap size | `share/gc/shared/gc_globals.hpp`

|===

{{< /wrapTable >}}


Point taken, I already knew https://twitter.com/chriswhocodes[Chris Newland]'s useful websites
but didn't visit them to use this option, *I should have !*

Anyway after all, I don't think `*RAMPercentage` flags are quite useful (or those
are used inadequately for this application ?!). For me they don't quite respect the _principle of
the least surprise_. We've seen these percentages lacks any consideration of how non-heap usage grow,
and the JVM didn't limit these zones according to the `cgroup` limits, which is unsettling, because
if they were, the JVM would have crashed with an ``OutOfMemoryError``s from these zones.

That being said, I believe that from now on it is actually just as ok if not better to prefer the usual
`-Xmx` flags for Java applications running in a container for now, especially with the
`JAVA_TOOL_OPTIONS` environment variable, and this a bit less work because it's not anymore necessary
to translate byte numbers in percentages but instead just use the actual max memory.


== Take away

* Use `JAVA_TOOL_OPTIONS` in the image rather than setting memory in the `CMD` directive.
* `RSS`, the amount of physical memory that is allocated & used by a process,
* `RSS` maybe more or inferior to committed memory of the JVM due to OS virtual memory management
* `/proc` filesystem and related tooling is great
* Java ~= heap + metaspace + off-heap (DirectBuffer + threads + compiled code + GC data + ...)
* Using `Xmx` in a container is still a very good choice compared to `MaxRAMPercentage`




'''
'''
'''
'''
'''
'''




   __
  /\ \
  \ \ \____    ___     ___   __  __    ____
   \ \ '__`\  / __`\ /' _ `\/\ \/\ \  /',__\
    \ \ \L\ \/\ \L\ \/\ \/\ \ \ \_\ \/\__, `\
     \ \_,__/\ \____/\ \_\ \_\ \____/\/\____/
      \/___/  \/___/  \/_/\/_/\/___/  \/___/


== Bonus

The main topic of blog post is over, but as it was interesting to look at this problem
with some `cgroup` knowledge and Linux memory understanding, so I wrapped some
information that was nice to refresh and explore.

=== Interpreting cgroup's memory (cgroup v1)

Before going further I'd like o mention that the Linux kernel documentation on
https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt[cgroup v1]
is a very good start.

In our case let's see what `cgroup` have to say inside our container. A lot of interesting
bits are available in the `/sys/fs/cgroup`, those are not process specific.
They may help tackle issue with memory not directly related with the process itself:

.memory.stat
[source, bash]
----
❯ kubectl exec -it --container=java-app deployment/java-app -- cat /sys/fs/cgroup/memory/memory.stat
cache 57434112 <7>
rss 4822343680 <1>
rss_huge 0
shmem 0
mapped_file 0
dirty 0
writeback 0
swap 0 <6>
pgpgin 7918680
pgpgout 6726903
pgfault 7682598
pgmajfault 0
pgmajfault_s 0
pgmajfault_a 0
pgmajfault_f 0
inactive_anon 0 <2>
active_anon 4823887872 <3>
inactive_file 58806272 <4>
active_file 188416 <5>
unevictable 0
hierarchical_memory_limit 5368709120
hierarchical_memsw_limit 5368709120
total_cache 57434112
total_rss 4822343680
total_rss_huge 0
total_shmem 0
total_mapped_file 0
total_dirty 0
total_writeback 0
total_swap 0
total_pgpgin 7918680
total_pgpgout 6726903
total_pgfault 7682598
total_pgmajfault 0
total_pgmajfault_s 0
total_pgmajfault_a 0
total_pgmajfault_f 0
total_inactive_anon 0
total_active_anon 4823887872
total_inactive_file 58806272
total_active_file 188416
total_unevictable 0
----
<1> rss of the processes, anonymous memory and swap cache, without `tmpfs` (shmem) (~`4.49 GiB`)
<2> anonymous memory and swap cache on active LRU list, with `tmpfs` (shmem)
<3> anonymous memory and swap cache on inactive LRU list, with `tmpfs` (shmem) (~`4.49 GiB`)
<4> file-backed memory on inactive LRU list, in bytes (~`56 MiB`)
<5> file-backed memory on active LRU list, in bytes (~`184 KiB`)
<6> swap usage, `0` is the only good value for java
<7> page cache memory (~`54.8 MiB`)

.From the https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-memory[RHEL6 documentation]
****
When you interpret the values reported by memory.stat, note how the various statistics inter-relate:

* `active_anon` + `inactive_anon` = anonymous memory + file cache for tmpfs + swap cache

Therefore, `active_anon` + `inactive_anon` ≠ rss, because rss does not include tmpfs.

* `active_file` + `inactive_file` = cache - size of tmpfs
****

There other memory settings to look at, some of these are being looked upon by the JVM
to understand the contraint of the cgroup.

.memory usage and limits
[source, bash]
----
cat /sys/fs/cgroup/memory/memory.{usage_in_bytes,limit_in_bytes,memsw.usage_in_bytes,memsw.limit_in_bytes}
4944756736 <1>
5368709120 <2>
4944748544 <3>
5368709120 <4>
----
<1> current memory usage ~`4.61 GiB`, but for the whole memory it's recommended to read cache+rss+swap values in `memory.stat`
<2> limit of the memory resource (~`5 GiB`)
<3> current memory and swap usage (~`4.61 GiB`)
<4> limit on memory and swap (~`5 GiB`)

Note the `memory.limit_in_bytes` and `memory.memsw.limit_in_bytes` values are the same,
that means that the processes in the cgroup can use all the memory before swaping,
however it is not impossible for the process to be use the swap before this limit is reached.

In fact due to the OS `swapiness` value the kernel may try to reclaim memory from RAM and put
on the swap.
There are other parameters related to the kernel and tcp allocations.

On the swapiness side, it's possible to change that in the cgroup as well.

.memory.swapiness
[source, bash]
----
cat /proc/sys/vm/swappiness <1>
60
cat /sys/fs/cgroup/memory/memory.swappiness <2>
60
----
<1> OS `swapiness`
<2> cgroup `swapiness`, here the setting is unchanged.

By the way that high `swappiness` is a bad for applications with GC like the JVM.

AS mentioned earlier the JVM look for some values in `memory.limit_in_bytes` and `memory.usage_in_bytes`,
but not only, let's find out with this logger :

.log container details
[source]
----
-Xlog:os,os+container=trace:file=/gclogs/%t-os-container.log:time,uptime,tags,level
----



.output
[source]
----
$ head -n 200 /logs/2020-05-22_22-28-32-os-container.log
[2020-05-22T23:17:44.775+0000][0.001s][trace][os,container] OSContainer::init: Initializing Container Support
[2020-05-22T23:17:44.776+0000][0.001s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes <1>
[2020-05-22T23:17:44.776+0000][0.001s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.776+0000][0.001s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us <2>
[2020-05-22T23:17:44.776+0000][0.001s][trace][os,container] CPU Quota is: 800000
[2020-05-22T23:17:44.776+0000][0.001s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us <3>
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Period is: 100000
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu,cpuacct/cpu.shares <4>
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Shares is: 3072
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Quota count based on quota/period: 8
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Share count based on shares: 3
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] OSContainer::active_processor_count: 8
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Quota is: 800000
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Period is: 100000
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu,cpuacct/cpu.shares
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Shares is: 3072
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Quota count based on quota/period: 8
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] CPU Share count based on shares: 3
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] OSContainer::active_processor_count: 8
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T23:17:44.776+0000][0.002s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.776+0000][0.002s][info ][os          ] Use of CLOCK_MONOTONIC is supported
[2020-05-22T23:17:44.776+0000][0.002s][info ][os          ] Use of pthread_condattr_setclock is supported
[2020-05-22T23:17:44.776+0000][0.002s][info ][os          ] Relative timed-wait using pthread_cond_timedwait is associated with CLOCK_MONOTONIC
[2020-05-22T23:17:44.776+0000][0.002s][info ][os          ] HotSpot is running with glibc 2.28, NPTL 2.28
[2020-05-22T23:17:44.776+0000][0.002s][info ][os          ] SafePoint Polling address, bad (protected) page:0x00007f3b2efcf000, good (unprotected) page:0x00007f3b2efd0000
[2020-05-22T23:17:44.777+0000][0.002s][info ][os,thread   ] Thread attached (tid: 2260, pthread id: 139892140738304).
[2020-05-22T23:17:44.777+0000][0.003s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libzip.so
[2020-05-22T23:17:44.777+0000][0.003s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libzip.so was successful
[2020-05-22T23:17:44.777+0000][0.003s][info ][os          ] attempting shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libjimage.so
[2020-05-22T23:17:44.777+0000][0.003s][info ][os          ] shared library load of /usr/lib/jvm/java-11-amazon-corretto/lib/libjimage.so was successful
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] CPU Quota is: 800000
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] CPU Period is: 100000
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu,cpuacct/cpu.shares
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] CPU Shares is: 3072
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] CPU Quota count based on quota/period: 8
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] CPU Share count based on shares: 3
[2020-05-22T23:17:44.777+0000][0.003s][trace][os,container] OSContainer::active_processor_count: 8
[2020-05-22T23:17:44.778+0000][0.004s][info ][os,cpu      ] CPU:total 32 (initial active 8) (16 cores per cpu, 2 threads per core) family 6 model 85 stepping 3, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, rtm, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx, fma
[2020-05-22T23:17:44.778+0000][0.004s][info ][os,cpu      ] CPU Model and flags from /proc/cpuinfo:
[2020-05-22T23:17:44.778+0000][0.004s][info ][os,cpu      ] model name  : Intel(R) Xeon(R) CPU @ 2.00GHz
[2020-05-22T23:17:44.778+0000][0.004s][info ][os,cpu      ] flags               : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities
[2020-05-22T23:17:44.779+0000][0.005s][info ][os,thread   ] Thread started (pthread id: 139892128659200, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.779+0000][0.005s][info ][os,thread   ] Thread is alive (tid: 2261, pthread id: 139892128659200).
[2020-05-22T23:17:44.780+0000][0.005s][info ][os,thread   ] Thread started (pthread id: 139891649345280, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.780+0000][0.006s][info ][os,thread   ] Thread is alive (tid: 2262, pthread id: 139891649345280).
[2020-05-22T23:17:44.780+0000][0.006s][info ][os,thread   ] Thread started (pthread id: 139891430127360, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.780+0000][0.006s][info ][os,thread   ] Thread is alive (tid: 2263, pthread id: 139891430127360).
[2020-05-22T23:17:44.817+0000][0.043s][info ][os,thread   ] Thread started (pthread id: 139891387094784, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.817+0000][0.043s][info ][os,thread   ] Thread is alive (tid: 2264, pthread id: 139891387094784).
[2020-05-22T23:17:44.818+0000][0.043s][info ][os,thread   ] Thread started (pthread id: 139891386038016, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.818+0000][0.043s][info ][os,thread   ] Thread is alive (tid: 2265, pthread id: 139891386038016).
[2020-05-22T23:17:44.835+0000][0.060s][info ][os,thread   ] Thread started (pthread id: 139891384080128, attributes: stacksize: 1024k, guardsize: 4k, detached).
[2020-05-22T23:17:44.835+0000][0.060s][info ][os,thread   ] Thread is alive (tid: 2266, pthread id: 139891384080128).
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] Path to /cpu.cfs_quota_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] CPU Quota is: 800000
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] Path to /cpu.cfs_period_us is /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] CPU Period is: 100000
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] Path to /cpu.shares is /sys/fs/cgroup/cpu,cpuacct/cpu.shares
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] CPU Shares is: 3072
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] CPU Quota count based on quota/period: 8
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] CPU Share count based on shares: 3
[2020-05-22T23:17:44.840+0000][0.065s][trace][os,container] OSContainer::active_processor_count: 8
[2020-05-22T23:17:44.841+0000][0.067s][info ][os,thread   ] Thread started (pthread id: 139891383023360, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.841+0000][0.067s][info ][os,thread   ] Thread is alive (tid: 2267, pthread id: 139891383023360).
[2020-05-22T23:17:44.842+0000][0.067s][info ][os,thread   ] Thread started (pthread id: 139891381970688, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.842+0000][0.067s][info ][os,thread   ] Thread is alive (tid: 2268, pthread id: 139891381970688).
[2020-05-22T23:17:44.851+0000][0.077s][info ][os,thread   ] Thread started (pthread id: 139891168560896, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.851+0000][0.077s][info ][os,thread   ] Thread is alive (tid: 2269, pthread id: 139891168560896).
[2020-05-22T23:17:44.852+0000][0.077s][info ][os,thread   ] Thread started (pthread id: 139891167508224, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.852+0000][0.077s][info ][os,thread   ] Thread is alive (tid: 2270, pthread id: 139891167508224).
[2020-05-22T23:17:44.852+0000][0.078s][info ][os,thread   ] Thread started (pthread id: 139891166455552, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.852+0000][0.078s][info ][os,thread   ] Thread is alive (tid: 2271, pthread id: 139891166455552).
[2020-05-22T23:17:44.852+0000][0.078s][info ][os,thread   ] Thread started (pthread id: 139891165402880, attributes: stacksize: 1024k, guardsize: 0k, detached).
[2020-05-22T23:17:44.853+0000][0.078s][info ][os,thread   ] Thread is alive (tid: 2272, pthread id: 139891165402880).
[2020-05-22T23:17:44.858+0000][0.084s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes <1>
[2020-05-22T23:17:44.858+0000][0.084s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.858+0000][0.084s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes <5>
[2020-05-22T23:17:44.858+0000][0.084s][trace][os,container] Memory Usage is: 4583374848
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Memory Usage is: 4583374848
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Path to /memory.limit_in_bytes is /sys/fs/cgroup/memory/memory.limit_in_bytes
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Memory Limit is: 5368709120
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Path to /memory.usage_in_bytes is /sys/fs/cgroup/memory/memory.usage_in_bytes
[2020-05-22T23:17:44.859+0000][0.084s][trace][os,container] Memory Usage is: 4583505920
...
----
<1> `/sys/fs/cgroup/memory/memory.limit_in_bytes` = 5368709120 bytes, in the k8s deployment object `spec.containers[0].resources.limits.memory: 5Gi`
<2> `/sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us` = 800000, in the k8s deployment object `spec.containers[0].resources.limits.cpu: 8`
<3> `/sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us` = 100000, 1/10 of a second, the minimal time slice the process can be scheduled on the CPU,
<4> `/sys/fs/cgroup/cpu,cpuacct/cpu.shares` = 3072, in the k8s deployment object this comes from `spec.containers[0].resources.requests.cpu: 3`
<5> `/sys/fs/cgroup/memory/memory.usage_in_bytes` = 4583374848 bytes => `4.27 GiB`

Kubernetes documentation was lacking a bit in that regard, but I found
https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b[this blog post that
explained a bit better how cpu resource limits work in kubernetes].
And anyway the official documentation is https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run[here].





////

https://pangin.pro/posts/stack-overflow-handling

https://stackoverflow.com/questions/25309748/what-is-thread-stack-size-option-xss-given-to-jvm-why-does-it-have-a-limit-of[What is thread stack size option(-Xss) given to jvm? Why does it have a limit of atleast 68k in a windows pc?]

Memory footprint of a Java process by Andrei Pangin
https://www.youtube.com/watch?v=c755fFv1Rnk

////